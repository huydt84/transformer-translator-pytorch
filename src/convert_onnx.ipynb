{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\transformer-translator-pytorch\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "import sentencepiece\n",
    "import torch\n",
    "from constants import *\n",
    "from custom_data import *\n",
    "from transformer import *\n",
    "from data_structure import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(64000, 64000)\n",
    "checkpoint = torch.load(\"saved_model\\\\best_ckpt.tar\", map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (src_embedding): Embedding(64000, 512)\n",
       "  (trg_embedding): Embedding(64000, 512)\n",
       "  (positional_encoder): PositionalEncoder()\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x EncoderLayer(\n",
       "        (layer_norm_1): LayerNormalization(\n",
       "          (layer): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (multihead_attention): MultiheadAttention(\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (attn_softmax): Softmax(dim=-1)\n",
       "          (w_0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (drop_out_1): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm_2): LayerNormalization(\n",
       "          (layer): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (feed_forward): FeedFowardLayer(\n",
       "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (drop_out_2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNormalization(\n",
       "      (layer): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x DecoderLayer(\n",
       "        (layer_norm_1): LayerNormalization(\n",
       "          (layer): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (masked_multihead_attention): MultiheadAttention(\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (attn_softmax): Softmax(dim=-1)\n",
       "          (w_0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (drop_out_1): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm_2): LayerNormalization(\n",
       "          (layer): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (multihead_attention): MultiheadAttention(\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (attn_softmax): Softmax(dim=-1)\n",
       "          (w_0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (drop_out_2): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm_3): LayerNormalization(\n",
       "          (layer): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (feed_forward): FeedFowardLayer(\n",
       "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (drop_out_3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNormalization(\n",
       "      (layer): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (output_linear): Linear(in_features=512, out_features=64000, bias=True)\n",
       "  (softmax): LogSoftmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_sp = spm.SentencePieceProcessor()\n",
    "trg_sp = spm.SentencePieceProcessor()\n",
    "src_sp.load(f\"{SP_DIR}/{src_model_prefix}.model\")\n",
    "trg_sp.load(f\"{SP_DIR}/{trg_model_prefix}.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = \"Hello my friend\"\n",
    "tokenized = src_sp.EncodeAsIds(input_sentence)\n",
    "src = torch.LongTensor(pad_or_truncate(tokenized)).unsqueeze(0).to(device) # (1, L)\n",
    "e_mask = (src != pad_id).unsqueeze(1).to(device) # (1, 1, L)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 256, 512)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_embedding_part(model, src, save_path): \n",
    "    print(src.shape)\n",
    "    src_data = model.src_embedding(src) \n",
    "    # torch.onnx.export(model.src_embedding, \n",
    "    #                   src, \n",
    "    #                   save_path, \n",
    "    #                   export_params=True, \n",
    "    #                   opset_version=16, \n",
    "    #                   do_constant_folding=True, \n",
    "    #                   input_names=['src'], \n",
    "    #                   output_names=['src_data']) \n",
    "    return src_data.detach().numpy().shape\n",
    "\n",
    "src_data = convert_embedding_part(model, src, \"onnx_test\\\\src_embedding.onnx\")\n",
    "src_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Diagnostic Run torch.onnx.export version 2.0.0+cpu ==============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def convert_pe_part(model, src, save_path): \n",
    "    src_data2 = model.positional_encoder(src) \n",
    "    torch.onnx.export(model.positional_encoder, \n",
    "                      src, \n",
    "                      save_path, \n",
    "                      export_params=True, \n",
    "                      opset_version=16, \n",
    "                      do_constant_folding=True, \n",
    "                      input_names=['src_data'], \n",
    "                      output_names=['src_data2']) \n",
    "    return src_data2\n",
    "\n",
    "src_data2 = convert_pe_part(model, src_data, \"onnx_test\\\\positional_encoder.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Diagnostic Run torch.onnx.export version 2.0.0+cpu ==============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def convert_encoder_part(model, src, src_mask, save_path): \n",
    "    e_output = model.encoder(src, src_mask) \n",
    "    torch.onnx.export(model.encoder, \n",
    "                      (src, src_mask), \n",
    "                      save_path, \n",
    "                      export_params=True, \n",
    "                      opset_version=16, \n",
    "                      do_constant_folding=True, \n",
    "                      input_names=['src_data2', 'e_mask'], \n",
    "                      output_names=['e_output']) \n",
    "    return e_output\n",
    "\n",
    "e_output = convert_encoder_part(model, src_data2, e_mask, \"onnx_test\\\\encoder.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Diagnostic Run torch.onnx.export version 2.0.0+cpu ==============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "last_words = torch.LongTensor([pad_id] * seq_len) # (L)\n",
    "last_words[0] = sos_id # (L)\n",
    "cur_len = 1\n",
    "\n",
    "trg = last_words.unsqueeze(0)\n",
    "        \n",
    "d_mask = (last_words.unsqueeze(0) != pad_id).unsqueeze(1) # (1, 1, L)\n",
    "nopeak_mask = torch.ones([1, seq_len, seq_len], dtype=torch.bool)  # (1, L, L)\n",
    "nopeak_mask = torch.tril(nopeak_mask)  # (1, L, L) to triangular shape\n",
    "d_mask = d_mask & nopeak_mask\n",
    "\n",
    "def convert_embedding2_part(model, trg, save_path): \n",
    "    trg_embedded = model.trg_embedding(trg) \n",
    "    torch.onnx.export(model.trg_embedding, \n",
    "                      trg, \n",
    "                      save_path, \n",
    "                      export_params=True, \n",
    "                      opset_version=16, \n",
    "                      do_constant_folding=True, \n",
    "                      input_names=['trg'], \n",
    "                      output_names=['trg_embedded']) \n",
    "    return trg_embedded\n",
    "\n",
    "trg_embedded = convert_embedding2_part(model, trg, \"onnx_test\\\\trg_embedding.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_positional_encoded = model.positional_encoder(trg_embedded) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Diagnostic Run torch.onnx.export version 2.0.0+cpu ==============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def convert_decoder_part(model, trg_positional_encoded, e_output, e_mask, d_mask, save_path): \n",
    "    decoder_output = model.decoder(\n",
    "                trg_positional_encoded,\n",
    "                e_output,\n",
    "                e_mask,\n",
    "                d_mask\n",
    "            ) # (1, L, d_model) \n",
    "    torch.onnx.export(model.decoder, \n",
    "                      (trg_positional_encoded, e_output, e_mask, d_mask), \n",
    "                      save_path, \n",
    "                      export_params=True, \n",
    "                      opset_version=16, \n",
    "                      do_constant_folding=True, \n",
    "                      input_names=['trg_positional_encoded', 'e_output', 'e_mask', 'd_mask'], \n",
    "                      output_names=['decoder_output']) \n",
    "    return decoder_output\n",
    "\n",
    "decoder_output = convert_decoder_part(model, trg_positional_encoded, e_output, e_mask, d_mask, \"onnx_test\\\\decoder.onnx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and check model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "embed1 = onnx.load(\"onnx_test\\\\src_embedding.onnx\")\n",
    "embed2 = onnx.load(\"onnx_test\\\\trg_embedding.onnx\")\n",
    "pe = onnx.load(\"onnx_test\\\\positional_encoder.onnx\")\n",
    "encoder = onnx.load(\"onnx_test\\\\encoder.onnx\")\n",
    "decoder = onnx.load(\"onnx_test\\\\decoder.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx.checker.check_model(embed1)\n",
    "onnx.checker.check_model(embed2)\n",
    "onnx.checker.check_model(pe)\n",
    "onnx.checker.check_model(encoder)\n",
    "onnx.checker.check_model(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graph torch_jit (\\n  %src_data2[FLOAT, 1x256x512]\\n  %e_mask[BOOL, 1x1x256]\\n) initializers (\\n  %layers.0.layer_norm_1.layer.weight[FLOAT, 512]\\n  %layers.0.layer_norm_1.layer.bias[FLOAT, 512]\\n  %layers.0.multihead_attention.w_q.bias[FLOAT, 512]\\n  %layers.0.multihead_attention.w_k.bias[FLOAT, 512]\\n  %layers.0.multihead_attention.w_v.bias[FLOAT, 512]\\n  %layers.0.multihead_attention.w_0.bias[FLOAT, 512]\\n  %layers.0.layer_norm_2.layer.weight[FLOAT, 512]\\n  %layers.0.layer_norm_2.layer.bias[FLOAT, 512]\\n  %layers.0.feed_forward.linear_1.bias[FLOAT, 2048]\\n  %layers.0.feed_forward.linear_2.bias[FLOAT, 512]\\n  %layers.1.layer_norm_1.layer.weight[FLOAT, 512]\\n  %layers.1.layer_norm_1.layer.bias[FLOAT, 512]\\n  %layers.1.multihead_attention.w_q.bias[FLOAT, 512]\\n  %layers.1.multihead_attention.w_k.bias[FLOAT, 512]\\n  %layers.1.multihead_attention.w_v.bias[FLOAT, 512]\\n  %layers.1.multihead_attention.w_0.bias[FLOAT, 512]\\n  %layers.1.layer_norm_2.layer.weight[FLOAT, 512]\\n  %layers.1.layer_norm_2.layer.bias[FLOAT, 512]\\n  %layers.1.feed_forward.linear_1.bias[FLOAT, 2048]\\n  %layers.1.feed_forward.linear_2.bias[FLOAT, 512]\\n  %layers.2.layer_norm_1.layer.weight[FLOAT, 512]\\n  %layers.2.layer_norm_1.layer.bias[FLOAT, 512]\\n  %layers.2.multihead_attention.w_q.bias[FLOAT, 512]\\n  %layers.2.multihead_attention.w_k.bias[FLOAT, 512]\\n  %layers.2.multihead_attention.w_v.bias[FLOAT, 512]\\n  %layers.2.multihead_attention.w_0.bias[FLOAT, 512]\\n  %layers.2.layer_norm_2.layer.weight[FLOAT, 512]\\n  %layers.2.layer_norm_2.layer.bias[FLOAT, 512]\\n  %layers.2.feed_forward.linear_1.bias[FLOAT, 2048]\\n  %layers.2.feed_forward.linear_2.bias[FLOAT, 512]\\n  %layer_norm.layer.weight[FLOAT, 512]\\n  %layer_norm.layer.bias[FLOAT, 512]\\n  %onnx::MatMul_354[FLOAT, 512x512]\\n  %onnx::MatMul_370[FLOAT, 512x512]\\n  %onnx::MatMul_371[FLOAT, 512x512]\\n  %onnx::MatMul_376[FLOAT, 512x512]\\n  %onnx::MatMul_377[FLOAT, 512x2048]\\n  %onnx::MatMul_378[FLOAT, 2048x512]\\n  %onnx::MatMul_379[FLOAT, 512x512]\\n  %onnx::MatMul_395[FLOAT, 512x512]\\n  %onnx::MatMul_396[FLOAT, 512x512]\\n  %onnx::MatMul_401[FLOAT, 512x512]\\n  %onnx::MatMul_402[FLOAT, 512x2048]\\n  %onnx::MatMul_403[FLOAT, 2048x512]\\n  %onnx::MatMul_404[FLOAT, 512x512]\\n  %onnx::MatMul_420[FLOAT, 512x512]\\n  %onnx::MatMul_421[FLOAT, 512x512]\\n  %onnx::MatMul_426[FLOAT, 512x512]\\n  %onnx::MatMul_427[FLOAT, 512x2048]\\n  %onnx::MatMul_428[FLOAT, 2048x512]\\n) {\\n  %/layers.0/layer_norm_1/layer/ReduceMean_output_0 = ReduceMean[axes = [-1]](%src_data2)\\n  %/layers.0/layer_norm_1/layer/Sub_output_0 = Sub(%src_data2, %/layers.0/layer_norm_1/layer/ReduceMean_output_0)\\n  %/layers.0/layer_norm_1/layer/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.0/layer_norm_1/layer/Pow_output_0 = Pow(%/layers.0/layer_norm_1/layer/Sub_output_0, %/layers.0/layer_norm_1/layer/Constant_output_0)\\n  %/layers.0/layer_norm_1/layer/ReduceMean_1_output_0 = ReduceMean[axes = [-1]](%/layers.0/layer_norm_1/layer/Pow_output_0)\\n  %/layers.0/layer_norm_1/layer/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.0/layer_norm_1/layer/Add_output_0 = Add(%/layers.0/layer_norm_1/layer/ReduceMean_1_output_0, %/layers.0/layer_norm_1/layer/Constant_1_output_0)\\n  %/layers.0/layer_norm_1/layer/Sqrt_output_0 = Sqrt(%/layers.0/layer_norm_1/layer/Add_output_0)\\n  %/layers.0/layer_norm_1/layer/Div_output_0 = Div(%/layers.0/layer_norm_1/layer/Sub_output_0, %/layers.0/layer_norm_1/layer/Sqrt_output_0)\\n  %/layers.0/layer_norm_1/layer/Mul_output_0 = Mul(%/layers.0/layer_norm_1/layer/Div_output_0, %layers.0.layer_norm_1.layer.weight)\\n  %/layers.0/layer_norm_1/layer/Add_1_output_0 = Add(%/layers.0/layer_norm_1/layer/Mul_output_0, %layers.0.layer_norm_1.layer.bias)\\n  %/layers.0/multihead_attention/w_q/MatMul_output_0 = MatMul(%/layers.0/layer_norm_1/layer/Add_1_output_0, %onnx::MatMul_354)\\n  %/layers.0/multihead_attention/w_q/Add_output_0 = Add(%layers.0.multihead_attention.w_q.bias, %/layers.0/multihead_attention/w_q/MatMul_output_0)\\n  %/layers.0/multihead_attention/Constant_output_0 = Constant[value = <Tensor>]()\\n  %/layers.0/multihead_attention/Constant_1_output_0 = Constant[value = <Tensor>]()\\n  %/layers.0/multihead_attention/Constant_2_output_0 = Constant[value = <Tensor>]()\\n  %/layers.0/multihead_attention/Reshape_output_0 = Reshape[allowzero = 0](%/layers.0/multihead_attention/w_q/Add_output_0, %/layers.0/multihead_attention/Constant_output_0)\\n  %/layers.0/multihead_attention/w_k/MatMul_output_0 = MatMul(%/layers.0/layer_norm_1/layer/Add_1_output_0, %onnx::MatMul_370)\\n  %/layers.0/multihead_attention/w_k/Add_output_0 = Add(%layers.0.multihead_attention.w_k.bias, %/layers.0/multihead_attention/w_k/MatMul_output_0)\\n  %/layers.0/multihead_attention/Reshape_1_output_0 = Reshape[allowzero = 0](%/layers.0/multihead_attention/w_k/Add_output_0, %/layers.0/multihead_attention/Constant_1_output_0)\\n  %/layers.0/multihead_attention/w_v/MatMul_output_0 = MatMul(%/layers.0/layer_norm_1/layer/Add_1_output_0, %onnx::MatMul_371)\\n  %/layers.0/multihead_attention/w_v/Add_output_0 = Add(%layers.0.multihead_attention.w_v.bias, %/layers.0/multihead_attention/w_v/MatMul_output_0)\\n  %/layers.0/multihead_attention/Reshape_2_output_0 = Reshape[allowzero = 0](%/layers.0/multihead_attention/w_v/Add_output_0, %/layers.0/multihead_attention/Constant_2_output_0)\\n  %/layers.0/multihead_attention/Transpose_output_0 = Transpose[perm = [0, 2, 1, 3]](%/layers.0/multihead_attention/Reshape_output_0)\\n  %/layers.0/multihead_attention/Transpose_1_output_0 = Transpose[perm = [0, 2, 1, 3]](%/layers.0/multihead_attention/Reshape_2_output_0)\\n  %/layers.0/multihead_attention/Transpose_2_output_0 = Transpose[perm = [0, 2, 3, 1]](%/layers.0/multihead_attention/Reshape_1_output_0)\\n  %/layers.0/multihead_attention/MatMul_output_0 = MatMul(%/layers.0/multihead_attention/Transpose_output_0, %/layers.0/multihead_attention/Transpose_2_output_0)\\n  %/layers.0/multihead_attention/Constant_3_output_0 = Constant[value = <Tensor>]()\\n  %/layers.0/multihead_attention/Unsqueeze_output_0 = Unsqueeze(%e_mask, %/layers.0/multihead_attention/Constant_3_output_0)\\n  %/layers.0/multihead_attention/Cast_output_0 = Cast[to = 7](%/layers.0/multihead_attention/Unsqueeze_output_0)\\n  %/layers.0/multihead_attention/Constant_4_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.0/multihead_attention/Equal_output_0 = Equal(%/layers.0/multihead_attention/Cast_output_0, %/layers.0/multihead_attention/Constant_4_output_0)\\n  %/layers.0/multihead_attention/Constant_5_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.0/multihead_attention/Div_output_0 = Div(%/layers.0/multihead_attention/MatMul_output_0, %/layers.0/multihead_attention/Constant_5_output_0)\\n  %/layers.0/multihead_attention/Cast_1_output_0 = Cast[to = 9](%/layers.0/multihead_attention/Equal_output_0)\\n  %/layers.0/multihead_attention/Constant_6_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.0/multihead_attention/Where_output_0 = Where(%/layers.0/multihead_attention/Cast_1_output_0, %/layers.0/multihead_attention/Constant_6_output_0, %/layers.0/multihead_attention/Div_output_0)\\n  %/layers.0/multihead_attention/attn_softmax/Softmax_output_0 = Softmax[axis = -1](%/layers.0/multihead_attention/Where_output_0)\\n  %/layers.0/multihead_attention/MatMul_1_output_0 = MatMul(%/layers.0/multihead_attention/attn_softmax/Softmax_output_0, %/layers.0/multihead_attention/Transpose_1_output_0)\\n  %/layers.0/multihead_attention/Transpose_3_output_0 = Transpose[perm = [0, 2, 1, 3]](%/layers.0/multihead_attention/MatMul_1_output_0)\\n  %/layers.0/multihead_attention/Constant_7_output_0 = Constant[value = <Tensor>]()\\n  %/layers.0/multihead_attention/Reshape_3_output_0 = Reshape[allowzero = 0](%/layers.0/multihead_attention/Transpose_3_output_0, %/layers.0/multihead_attention/Constant_7_output_0)\\n  %/layers.0/multihead_attention/w_0/MatMul_output_0 = MatMul(%/layers.0/multihead_attention/Reshape_3_output_0, %onnx::MatMul_376)\\n  %/layers.0/multihead_attention/w_0/Add_output_0 = Add(%layers.0.multihead_attention.w_0.bias, %/layers.0/multihead_attention/w_0/MatMul_output_0)\\n  %/layers.0/Add_output_0 = Add(%src_data2, %/layers.0/multihead_attention/w_0/Add_output_0)\\n  %/layers.0/layer_norm_2/layer/ReduceMean_output_0 = ReduceMean[axes = [-1]](%/layers.0/Add_output_0)\\n  %/layers.0/layer_norm_2/layer/Sub_output_0 = Sub(%/layers.0/Add_output_0, %/layers.0/layer_norm_2/layer/ReduceMean_output_0)\\n  %/layers.0/layer_norm_2/layer/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.0/layer_norm_2/layer/Pow_output_0 = Pow(%/layers.0/layer_norm_2/layer/Sub_output_0, %/layers.0/layer_norm_2/layer/Constant_output_0)\\n  %/layers.0/layer_norm_2/layer/ReduceMean_1_output_0 = ReduceMean[axes = [-1]](%/layers.0/layer_norm_2/layer/Pow_output_0)\\n  %/layers.0/layer_norm_2/layer/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.0/layer_norm_2/layer/Add_output_0 = Add(%/layers.0/layer_norm_2/layer/ReduceMean_1_output_0, %/layers.0/layer_norm_2/layer/Constant_1_output_0)\\n  %/layers.0/layer_norm_2/layer/Sqrt_output_0 = Sqrt(%/layers.0/layer_norm_2/layer/Add_output_0)\\n  %/layers.0/layer_norm_2/layer/Div_output_0 = Div(%/layers.0/layer_norm_2/layer/Sub_output_0, %/layers.0/layer_norm_2/layer/Sqrt_output_0)\\n  %/layers.0/layer_norm_2/layer/Mul_output_0 = Mul(%/layers.0/layer_norm_2/layer/Div_output_0, %layers.0.layer_norm_2.layer.weight)\\n  %/layers.0/layer_norm_2/layer/Add_1_output_0 = Add(%/layers.0/layer_norm_2/layer/Mul_output_0, %layers.0.layer_norm_2.layer.bias)\\n  %/layers.0/feed_forward/linear_1/MatMul_output_0 = MatMul(%/layers.0/layer_norm_2/layer/Add_1_output_0, %onnx::MatMul_377)\\n  %/layers.0/feed_forward/linear_1/Add_output_0 = Add(%layers.0.feed_forward.linear_1.bias, %/layers.0/feed_forward/linear_1/MatMul_output_0)\\n  %/layers.0/feed_forward/relu/Relu_output_0 = Relu(%/layers.0/feed_forward/linear_1/Add_output_0)\\n  %/layers.0/feed_forward/linear_2/MatMul_output_0 = MatMul(%/layers.0/feed_forward/relu/Relu_output_0, %onnx::MatMul_378)\\n  %/layers.0/feed_forward/linear_2/Add_output_0 = Add(%layers.0.feed_forward.linear_2.bias, %/layers.0/feed_forward/linear_2/MatMul_output_0)\\n  %/layers.0/Add_1_output_0 = Add(%/layers.0/Add_output_0, %/layers.0/feed_forward/linear_2/Add_output_0)\\n  %/layers.1/layer_norm_1/layer/ReduceMean_output_0 = ReduceMean[axes = [-1]](%/layers.0/Add_1_output_0)\\n  %/layers.1/layer_norm_1/layer/Sub_output_0 = Sub(%/layers.0/Add_1_output_0, %/layers.1/layer_norm_1/layer/ReduceMean_output_0)\\n  %/layers.1/layer_norm_1/layer/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.1/layer_norm_1/layer/Pow_output_0 = Pow(%/layers.1/layer_norm_1/layer/Sub_output_0, %/layers.1/layer_norm_1/layer/Constant_output_0)\\n  %/layers.1/layer_norm_1/layer/ReduceMean_1_output_0 = ReduceMean[axes = [-1]](%/layers.1/layer_norm_1/layer/Pow_output_0)\\n  %/layers.1/layer_norm_1/layer/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.1/layer_norm_1/layer/Add_output_0 = Add(%/layers.1/layer_norm_1/layer/ReduceMean_1_output_0, %/layers.1/layer_norm_1/layer/Constant_1_output_0)\\n  %/layers.1/layer_norm_1/layer/Sqrt_output_0 = Sqrt(%/layers.1/layer_norm_1/layer/Add_output_0)\\n  %/layers.1/layer_norm_1/layer/Div_output_0 = Div(%/layers.1/layer_norm_1/layer/Sub_output_0, %/layers.1/layer_norm_1/layer/Sqrt_output_0)\\n  %/layers.1/layer_norm_1/layer/Mul_output_0 = Mul(%/layers.1/layer_norm_1/layer/Div_output_0, %layers.1.layer_norm_1.layer.weight)\\n  %/layers.1/layer_norm_1/layer/Add_1_output_0 = Add(%/layers.1/layer_norm_1/layer/Mul_output_0, %layers.1.layer_norm_1.layer.bias)\\n  %/layers.1/multihead_attention/w_q/MatMul_output_0 = MatMul(%/layers.1/layer_norm_1/layer/Add_1_output_0, %onnx::MatMul_379)\\n  %/layers.1/multihead_attention/w_q/Add_output_0 = Add(%layers.1.multihead_attention.w_q.bias, %/layers.1/multihead_attention/w_q/MatMul_output_0)\\n  %/layers.1/multihead_attention/Constant_output_0 = Constant[value = <Tensor>]()\\n  %/layers.1/multihead_attention/Constant_1_output_0 = Constant[value = <Tensor>]()\\n  %/layers.1/multihead_attention/Constant_2_output_0 = Constant[value = <Tensor>]()\\n  %/layers.1/multihead_attention/Reshape_output_0 = Reshape[allowzero = 0](%/layers.1/multihead_attention/w_q/Add_output_0, %/layers.1/multihead_attention/Constant_output_0)\\n  %/layers.1/multihead_attention/w_k/MatMul_output_0 = MatMul(%/layers.1/layer_norm_1/layer/Add_1_output_0, %onnx::MatMul_395)\\n  %/layers.1/multihead_attention/w_k/Add_output_0 = Add(%layers.1.multihead_attention.w_k.bias, %/layers.1/multihead_attention/w_k/MatMul_output_0)\\n  %/layers.1/multihead_attention/Reshape_1_output_0 = Reshape[allowzero = 0](%/layers.1/multihead_attention/w_k/Add_output_0, %/layers.1/multihead_attention/Constant_1_output_0)\\n  %/layers.1/multihead_attention/w_v/MatMul_output_0 = MatMul(%/layers.1/layer_norm_1/layer/Add_1_output_0, %onnx::MatMul_396)\\n  %/layers.1/multihead_attention/w_v/Add_output_0 = Add(%layers.1.multihead_attention.w_v.bias, %/layers.1/multihead_attention/w_v/MatMul_output_0)\\n  %/layers.1/multihead_attention/Reshape_2_output_0 = Reshape[allowzero = 0](%/layers.1/multihead_attention/w_v/Add_output_0, %/layers.1/multihead_attention/Constant_2_output_0)\\n  %/layers.1/multihead_attention/Transpose_output_0 = Transpose[perm = [0, 2, 1, 3]](%/layers.1/multihead_attention/Reshape_output_0)\\n  %/layers.1/multihead_attention/Transpose_1_output_0 = Transpose[perm = [0, 2, 1, 3]](%/layers.1/multihead_attention/Reshape_2_output_0)\\n  %/layers.1/multihead_attention/Transpose_2_output_0 = Transpose[perm = [0, 2, 3, 1]](%/layers.1/multihead_attention/Reshape_1_output_0)\\n  %/layers.1/multihead_attention/MatMul_output_0 = MatMul(%/layers.1/multihead_attention/Transpose_output_0, %/layers.1/multihead_attention/Transpose_2_output_0)\\n  %/layers.1/multihead_attention/Constant_3_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.1/multihead_attention/Div_output_0 = Div(%/layers.1/multihead_attention/MatMul_output_0, %/layers.1/multihead_attention/Constant_3_output_0)\\n  %/layers.1/multihead_attention/Cast_output_0 = Cast[to = 9](%/layers.0/multihead_attention/Equal_output_0)\\n  %/layers.1/multihead_attention/Constant_4_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.1/multihead_attention/Where_output_0 = Where(%/layers.1/multihead_attention/Cast_output_0, %/layers.1/multihead_attention/Constant_4_output_0, %/layers.1/multihead_attention/Div_output_0)\\n  %/layers.1/multihead_attention/attn_softmax/Softmax_output_0 = Softmax[axis = -1](%/layers.1/multihead_attention/Where_output_0)\\n  %/layers.1/multihead_attention/MatMul_1_output_0 = MatMul(%/layers.1/multihead_attention/attn_softmax/Softmax_output_0, %/layers.1/multihead_attention/Transpose_1_output_0)\\n  %/layers.1/multihead_attention/Transpose_3_output_0 = Transpose[perm = [0, 2, 1, 3]](%/layers.1/multihead_attention/MatMul_1_output_0)\\n  %/layers.1/multihead_attention/Constant_5_output_0 = Constant[value = <Tensor>]()\\n  %/layers.1/multihead_attention/Reshape_3_output_0 = Reshape[allowzero = 0](%/layers.1/multihead_attention/Transpose_3_output_0, %/layers.1/multihead_attention/Constant_5_output_0)\\n  %/layers.1/multihead_attention/w_0/MatMul_output_0 = MatMul(%/layers.1/multihead_attention/Reshape_3_output_0, %onnx::MatMul_401)\\n  %/layers.1/multihead_attention/w_0/Add_output_0 = Add(%layers.1.multihead_attention.w_0.bias, %/layers.1/multihead_attention/w_0/MatMul_output_0)\\n  %/layers.1/Add_output_0 = Add(%/layers.0/Add_1_output_0, %/layers.1/multihead_attention/w_0/Add_output_0)\\n  %/layers.1/layer_norm_2/layer/ReduceMean_output_0 = ReduceMean[axes = [-1]](%/layers.1/Add_output_0)\\n  %/layers.1/layer_norm_2/layer/Sub_output_0 = Sub(%/layers.1/Add_output_0, %/layers.1/layer_norm_2/layer/ReduceMean_output_0)\\n  %/layers.1/layer_norm_2/layer/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.1/layer_norm_2/layer/Pow_output_0 = Pow(%/layers.1/layer_norm_2/layer/Sub_output_0, %/layers.1/layer_norm_2/layer/Constant_output_0)\\n  %/layers.1/layer_norm_2/layer/ReduceMean_1_output_0 = ReduceMean[axes = [-1]](%/layers.1/layer_norm_2/layer/Pow_output_0)\\n  %/layers.1/layer_norm_2/layer/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.1/layer_norm_2/layer/Add_output_0 = Add(%/layers.1/layer_norm_2/layer/ReduceMean_1_output_0, %/layers.1/layer_norm_2/layer/Constant_1_output_0)\\n  %/layers.1/layer_norm_2/layer/Sqrt_output_0 = Sqrt(%/layers.1/layer_norm_2/layer/Add_output_0)\\n  %/layers.1/layer_norm_2/layer/Div_output_0 = Div(%/layers.1/layer_norm_2/layer/Sub_output_0, %/layers.1/layer_norm_2/layer/Sqrt_output_0)\\n  %/layers.1/layer_norm_2/layer/Mul_output_0 = Mul(%/layers.1/layer_norm_2/layer/Div_output_0, %layers.1.layer_norm_2.layer.weight)\\n  %/layers.1/layer_norm_2/layer/Add_1_output_0 = Add(%/layers.1/layer_norm_2/layer/Mul_output_0, %layers.1.layer_norm_2.layer.bias)\\n  %/layers.1/feed_forward/linear_1/MatMul_output_0 = MatMul(%/layers.1/layer_norm_2/layer/Add_1_output_0, %onnx::MatMul_402)\\n  %/layers.1/feed_forward/linear_1/Add_output_0 = Add(%layers.1.feed_forward.linear_1.bias, %/layers.1/feed_forward/linear_1/MatMul_output_0)\\n  %/layers.1/feed_forward/relu/Relu_output_0 = Relu(%/layers.1/feed_forward/linear_1/Add_output_0)\\n  %/layers.1/feed_forward/linear_2/MatMul_output_0 = MatMul(%/layers.1/feed_forward/relu/Relu_output_0, %onnx::MatMul_403)\\n  %/layers.1/feed_forward/linear_2/Add_output_0 = Add(%layers.1.feed_forward.linear_2.bias, %/layers.1/feed_forward/linear_2/MatMul_output_0)\\n  %/layers.1/Add_1_output_0 = Add(%/layers.1/Add_output_0, %/layers.1/feed_forward/linear_2/Add_output_0)\\n  %/layers.2/layer_norm_1/layer/ReduceMean_output_0 = ReduceMean[axes = [-1]](%/layers.1/Add_1_output_0)\\n  %/layers.2/layer_norm_1/layer/Sub_output_0 = Sub(%/layers.1/Add_1_output_0, %/layers.2/layer_norm_1/layer/ReduceMean_output_0)\\n  %/layers.2/layer_norm_1/layer/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.2/layer_norm_1/layer/Pow_output_0 = Pow(%/layers.2/layer_norm_1/layer/Sub_output_0, %/layers.2/layer_norm_1/layer/Constant_output_0)\\n  %/layers.2/layer_norm_1/layer/ReduceMean_1_output_0 = ReduceMean[axes = [-1]](%/layers.2/layer_norm_1/layer/Pow_output_0)\\n  %/layers.2/layer_norm_1/layer/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.2/layer_norm_1/layer/Add_output_0 = Add(%/layers.2/layer_norm_1/layer/ReduceMean_1_output_0, %/layers.2/layer_norm_1/layer/Constant_1_output_0)\\n  %/layers.2/layer_norm_1/layer/Sqrt_output_0 = Sqrt(%/layers.2/layer_norm_1/layer/Add_output_0)\\n  %/layers.2/layer_norm_1/layer/Div_output_0 = Div(%/layers.2/layer_norm_1/layer/Sub_output_0, %/layers.2/layer_norm_1/layer/Sqrt_output_0)\\n  %/layers.2/layer_norm_1/layer/Mul_output_0 = Mul(%/layers.2/layer_norm_1/layer/Div_output_0, %layers.2.layer_norm_1.layer.weight)\\n  %/layers.2/layer_norm_1/layer/Add_1_output_0 = Add(%/layers.2/layer_norm_1/layer/Mul_output_0, %layers.2.layer_norm_1.layer.bias)\\n  %/layers.2/multihead_attention/w_q/MatMul_output_0 = MatMul(%/layers.2/layer_norm_1/layer/Add_1_output_0, %onnx::MatMul_404)\\n  %/layers.2/multihead_attention/w_q/Add_output_0 = Add(%layers.2.multihead_attention.w_q.bias, %/layers.2/multihead_attention/w_q/MatMul_output_0)\\n  %/layers.2/multihead_attention/Constant_output_0 = Constant[value = <Tensor>]()\\n  %/layers.2/multihead_attention/Constant_1_output_0 = Constant[value = <Tensor>]()\\n  %/layers.2/multihead_attention/Constant_2_output_0 = Constant[value = <Tensor>]()\\n  %/layers.2/multihead_attention/Reshape_output_0 = Reshape[allowzero = 0](%/layers.2/multihead_attention/w_q/Add_output_0, %/layers.2/multihead_attention/Constant_output_0)\\n  %/layers.2/multihead_attention/w_k/MatMul_output_0 = MatMul(%/layers.2/layer_norm_1/layer/Add_1_output_0, %onnx::MatMul_420)\\n  %/layers.2/multihead_attention/w_k/Add_output_0 = Add(%layers.2.multihead_attention.w_k.bias, %/layers.2/multihead_attention/w_k/MatMul_output_0)\\n  %/layers.2/multihead_attention/Reshape_1_output_0 = Reshape[allowzero = 0](%/layers.2/multihead_attention/w_k/Add_output_0, %/layers.2/multihead_attention/Constant_1_output_0)\\n  %/layers.2/multihead_attention/w_v/MatMul_output_0 = MatMul(%/layers.2/layer_norm_1/layer/Add_1_output_0, %onnx::MatMul_421)\\n  %/layers.2/multihead_attention/w_v/Add_output_0 = Add(%layers.2.multihead_attention.w_v.bias, %/layers.2/multihead_attention/w_v/MatMul_output_0)\\n  %/layers.2/multihead_attention/Reshape_2_output_0 = Reshape[allowzero = 0](%/layers.2/multihead_attention/w_v/Add_output_0, %/layers.2/multihead_attention/Constant_2_output_0)\\n  %/layers.2/multihead_attention/Transpose_output_0 = Transpose[perm = [0, 2, 1, 3]](%/layers.2/multihead_attention/Reshape_output_0)\\n  %/layers.2/multihead_attention/Transpose_1_output_0 = Transpose[perm = [0, 2, 1, 3]](%/layers.2/multihead_attention/Reshape_2_output_0)\\n  %/layers.2/multihead_attention/Transpose_2_output_0 = Transpose[perm = [0, 2, 3, 1]](%/layers.2/multihead_attention/Reshape_1_output_0)\\n  %/layers.2/multihead_attention/MatMul_output_0 = MatMul(%/layers.2/multihead_attention/Transpose_output_0, %/layers.2/multihead_attention/Transpose_2_output_0)\\n  %/layers.2/multihead_attention/Constant_3_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.2/multihead_attention/Div_output_0 = Div(%/layers.2/multihead_attention/MatMul_output_0, %/layers.2/multihead_attention/Constant_3_output_0)\\n  %/layers.2/multihead_attention/Cast_output_0 = Cast[to = 9](%/layers.0/multihead_attention/Equal_output_0)\\n  %/layers.2/multihead_attention/Constant_4_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.2/multihead_attention/Where_output_0 = Where(%/layers.2/multihead_attention/Cast_output_0, %/layers.2/multihead_attention/Constant_4_output_0, %/layers.2/multihead_attention/Div_output_0)\\n  %/layers.2/multihead_attention/attn_softmax/Softmax_output_0 = Softmax[axis = -1](%/layers.2/multihead_attention/Where_output_0)\\n  %/layers.2/multihead_attention/MatMul_1_output_0 = MatMul(%/layers.2/multihead_attention/attn_softmax/Softmax_output_0, %/layers.2/multihead_attention/Transpose_1_output_0)\\n  %/layers.2/multihead_attention/Transpose_3_output_0 = Transpose[perm = [0, 2, 1, 3]](%/layers.2/multihead_attention/MatMul_1_output_0)\\n  %/layers.2/multihead_attention/Constant_5_output_0 = Constant[value = <Tensor>]()\\n  %/layers.2/multihead_attention/Reshape_3_output_0 = Reshape[allowzero = 0](%/layers.2/multihead_attention/Transpose_3_output_0, %/layers.2/multihead_attention/Constant_5_output_0)\\n  %/layers.2/multihead_attention/w_0/MatMul_output_0 = MatMul(%/layers.2/multihead_attention/Reshape_3_output_0, %onnx::MatMul_426)\\n  %/layers.2/multihead_attention/w_0/Add_output_0 = Add(%layers.2.multihead_attention.w_0.bias, %/layers.2/multihead_attention/w_0/MatMul_output_0)\\n  %/layers.2/Add_output_0 = Add(%/layers.1/Add_1_output_0, %/layers.2/multihead_attention/w_0/Add_output_0)\\n  %/layers.2/layer_norm_2/layer/ReduceMean_output_0 = ReduceMean[axes = [-1]](%/layers.2/Add_output_0)\\n  %/layers.2/layer_norm_2/layer/Sub_output_0 = Sub(%/layers.2/Add_output_0, %/layers.2/layer_norm_2/layer/ReduceMean_output_0)\\n  %/layers.2/layer_norm_2/layer/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.2/layer_norm_2/layer/Pow_output_0 = Pow(%/layers.2/layer_norm_2/layer/Sub_output_0, %/layers.2/layer_norm_2/layer/Constant_output_0)\\n  %/layers.2/layer_norm_2/layer/ReduceMean_1_output_0 = ReduceMean[axes = [-1]](%/layers.2/layer_norm_2/layer/Pow_output_0)\\n  %/layers.2/layer_norm_2/layer/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.2/layer_norm_2/layer/Add_output_0 = Add(%/layers.2/layer_norm_2/layer/ReduceMean_1_output_0, %/layers.2/layer_norm_2/layer/Constant_1_output_0)\\n  %/layers.2/layer_norm_2/layer/Sqrt_output_0 = Sqrt(%/layers.2/layer_norm_2/layer/Add_output_0)\\n  %/layers.2/layer_norm_2/layer/Div_output_0 = Div(%/layers.2/layer_norm_2/layer/Sub_output_0, %/layers.2/layer_norm_2/layer/Sqrt_output_0)\\n  %/layers.2/layer_norm_2/layer/Mul_output_0 = Mul(%/layers.2/layer_norm_2/layer/Div_output_0, %layers.2.layer_norm_2.layer.weight)\\n  %/layers.2/layer_norm_2/layer/Add_1_output_0 = Add(%/layers.2/layer_norm_2/layer/Mul_output_0, %layers.2.layer_norm_2.layer.bias)\\n  %/layers.2/feed_forward/linear_1/MatMul_output_0 = MatMul(%/layers.2/layer_norm_2/layer/Add_1_output_0, %onnx::MatMul_427)\\n  %/layers.2/feed_forward/linear_1/Add_output_0 = Add(%layers.2.feed_forward.linear_1.bias, %/layers.2/feed_forward/linear_1/MatMul_output_0)\\n  %/layers.2/feed_forward/relu/Relu_output_0 = Relu(%/layers.2/feed_forward/linear_1/Add_output_0)\\n  %/layers.2/feed_forward/linear_2/MatMul_output_0 = MatMul(%/layers.2/feed_forward/relu/Relu_output_0, %onnx::MatMul_428)\\n  %/layers.2/feed_forward/linear_2/Add_output_0 = Add(%layers.2.feed_forward.linear_2.bias, %/layers.2/feed_forward/linear_2/MatMul_output_0)\\n  %/layers.2/Add_1_output_0 = Add(%/layers.2/Add_output_0, %/layers.2/feed_forward/linear_2/Add_output_0)\\n  %/layer_norm/layer/ReduceMean_output_0 = ReduceMean[axes = [-1]](%/layers.2/Add_1_output_0)\\n  %/layer_norm/layer/Sub_output_0 = Sub(%/layers.2/Add_1_output_0, %/layer_norm/layer/ReduceMean_output_0)\\n  %/layer_norm/layer/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layer_norm/layer/Pow_output_0 = Pow(%/layer_norm/layer/Sub_output_0, %/layer_norm/layer/Constant_output_0)\\n  %/layer_norm/layer/ReduceMean_1_output_0 = ReduceMean[axes = [-1]](%/layer_norm/layer/Pow_output_0)\\n  %/layer_norm/layer/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layer_norm/layer/Add_output_0 = Add(%/layer_norm/layer/ReduceMean_1_output_0, %/layer_norm/layer/Constant_1_output_0)\\n  %/layer_norm/layer/Sqrt_output_0 = Sqrt(%/layer_norm/layer/Add_output_0)\\n  %/layer_norm/layer/Div_output_0 = Div(%/layer_norm/layer/Sub_output_0, %/layer_norm/layer/Sqrt_output_0)\\n  %/layer_norm/layer/Mul_output_0 = Mul(%/layer_norm/layer/Div_output_0, %layer_norm.layer.weight)\\n  %e_output = Add(%/layer_norm/layer/Mul_output_0, %layer_norm.layer.bias)\\n  return %e_output\\n}'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx.helper.printable_graph(encoder.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "\n",
    "src_embed = onnxruntime.InferenceSession(\"onnx_test\\\\src_embedding.onnx\")\n",
    "trg_embed = onnxruntime.InferenceSession(\"onnx_test\\\\trg_embedding.onnx\")\n",
    "pe = onnxruntime.InferenceSession(\"onnx_test\\\\positional_encoder.onnx\")\n",
    "encoder = onnxruntime.InferenceSession(\"onnx_test\\\\encoder.onnx\")\n",
    "decoder = onnxruntime.InferenceSession(\"onnx_test\\\\decoder.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 256)\n",
      "(1, 256, 512)\n",
      "(1, 256, 512)\n",
      "(1, 256, 512)\n"
     ]
    }
   ],
   "source": [
    "class Translator():\n",
    "    def __init__(self, session) -> None:\n",
    "        self.src_embed, self.trg_embed, self.pe, self.encoder, self.decoder = session\n",
    "        self.src_sp = spm.SentencePieceProcessor()\n",
    "        self.trg_sp = spm.SentencePieceProcessor()\n",
    "        self.src_sp.load(f\"{SP_DIR}/{src_model_prefix}.model\")\n",
    "        self.trg_sp.load(f\"{SP_DIR}/{trg_model_prefix}.model\")\n",
    "        \n",
    "\n",
    "    def translate(self, input_sentence, session, method=\"greedy\"):\n",
    "        tokenized = self.src_sp.EncodeAsIds(input_sentence)\n",
    "        src = np.expand_dims(pad_or_truncate(tokenized), axis=0).astype('int64') # (1, L)\n",
    "        e_mask = np.expand_dims((src != pad_id), axis=1) # (1, 1, L)\n",
    "        print(src.shape)\n",
    "        \n",
    "        src_data_embed_input = { self.src_embed.get_inputs()[0].name: src }\n",
    "        src_data_embed = self.src_embed.run(None, src_data_embed_input)[0]\n",
    "        print(np.array(src_data_embed).shape)\n",
    "        \n",
    "        src_data_pe_input = { self.pe.get_inputs()[0].name: src_data_embed }\n",
    "        src_data_pe = self.pe.run(None, src_data_pe_input)[0]\n",
    "        print(np.array(src_data_pe).shape)\n",
    "        \n",
    "        e_output_input = { self.encoder.get_inputs()[0].name: src_data_pe, self.encoder.get_inputs()[1].name: e_mask}\n",
    "        e_output = self.encoder.run(None, e_output_input)[0]\n",
    "        print(np.array(e_output).shape)\n",
    "        \n",
    "        if method == 'greedy':\n",
    "            print(\"Greedy decoding selected.\")\n",
    "            result = self.greedy_search(e_output, e_mask, trg_sp)\n",
    "            \n",
    "    def greedy_search(self, e_output, e_mask, trg_sp):\n",
    "        last_words = torch.LongTensor([pad_id] * seq_len).to(device) # (L)\n",
    "        last_words[0] = sos_id # (L)\n",
    "        cur_len = 1\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            d_mask = (last_words.unsqueeze(0) != pad_id).unsqueeze(1).to(device) # (1, 1, L)\n",
    "            nopeak_mask = torch.ones([1, seq_len, seq_len], dtype=torch.bool).to(device)  # (1, L, L)\n",
    "            nopeak_mask = torch.tril(nopeak_mask)  # (1, L, L) to triangular shape\n",
    "            d_mask = d_mask & nopeak_mask  # (1, L, L) padding false\n",
    "\n",
    "            trg_embedded = self.model.trg_embedding(last_words.unsqueeze(0))\n",
    "            trg_positional_encoded = self.model.positional_encoder(trg_embedded)\n",
    "            decoder_output = self.model.decoder(\n",
    "                trg_positional_encoded,\n",
    "                e_output,\n",
    "                e_mask,\n",
    "                d_mask\n",
    "            ) # (1, L, d_model)\n",
    "\n",
    "            output = self.model.softmax(\n",
    "                self.model.output_linear(decoder_output)\n",
    "            ) # (1, L, trg_vocab_size)\n",
    "\n",
    "            output = torch.argmax(output, dim=-1) # (1, L)\n",
    "            last_word_id = output[0][i].item()\n",
    "            \n",
    "            if i < seq_len-1:\n",
    "                last_words[i+1] = last_word_id\n",
    "                cur_len += 1\n",
    "            \n",
    "            if last_word_id == eos_id:\n",
    "                break\n",
    "\n",
    "        if last_words[-1].item() == pad_id:\n",
    "            decoded_output = last_words[1:cur_len].tolist()\n",
    "        else:\n",
    "            decoded_output = last_words[1:].tolist()\n",
    "        decoded_output = trg_sp.decode_ids(decoded_output)\n",
    "        \n",
    "        return decoded_output\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_onnx(img, session, max_seq_length=128, sos_token=1, eos_token=2):\n",
    "    \"\"\"data: BxCxHxW\"\"\"\n",
    "    cnn_session, encoder_session, decoder_session = session\n",
    "    \n",
    "    # create cnn input\n",
    "    cnn_input = {cnn_session.get_inputs()[0].name: img}\n",
    "    src = cnn_session.run(None, cnn_input)\n",
    "    \n",
    "    # create encoder input\n",
    "    encoder_input = {encoder_session.get_inputs()[0].name: src[0]}\n",
    "    encoder_outputs, hidden = encoder_session.run(None, encoder_input)\n",
    "    translated_sentence = [[sos_token] * len(img)]\n",
    "    max_length = 0\n",
    "\n",
    "    while max_length <= max_seq_length and not all(\n",
    "        np.any(np.asarray(translated_sentence).T == eos_token, axis=1)\n",
    "    ):\n",
    "        tgt_inp = translated_sentence\n",
    "        decoder_input = {decoder_session.get_inputs()[0].name: tgt_inp[-1], decoder_session.get_inputs()[1].name: hidden, decoder_session.get_inputs()[2].name: encoder_outputs}\n",
    "\n",
    "        output, hidden, _ = decoder_session.run(None, decoder_input)\n",
    "        output = np.expand_dims(output, axis=1)\n",
    "        output = torch.Tensor(output)\n",
    "\n",
    "        values, indices = torch.topk(output, 1)\n",
    "        indices = indices[:, -1, 0]\n",
    "        indices = indices.tolist()\n",
    "\n",
    "        translated_sentence.append(indices)\n",
    "        max_length += 1\n",
    "\n",
    "        del output\n",
    "\n",
    "    translated_sentence = np.asarray(translated_sentence).T\n",
    "\n",
    "    return translated_sentence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
