{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dinh.trong.huy/nmt-data-envija/transformer-translator-pytorch\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sentencepiece as spm\n",
    "import numpy as np\n",
    "from constants import *\n",
    "from data_structure import *\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_src(text_list):\n",
    "    tokenized_list = []\n",
    "    for text in tqdm(text_list):\n",
    "        tokenized = src_sp.EncodeAsIds(text.strip())\n",
    "        tokenized_list.append(len(tokenized)+1)\n",
    "\n",
    "    return tokenized_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_truncate(tokenized_text):\n",
    "    if len(tokenized_text) < seq_len:\n",
    "        left = seq_len - len(tokenized_text)\n",
    "        padding = [pad_id] * left\n",
    "        tokenized_text += padding\n",
    "    else:\n",
    "        tokenized_text = tokenized_text[:seq_len]\n",
    "\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_sp = spm.SentencePieceProcessor()\n",
    "trg_sp = spm.SentencePieceProcessor()\n",
    "src_sp.load(f\"{SP_DIR}/{src_model_prefix}.model\")\n",
    "trg_sp.load(f\"{SP_DIR}/{trg_model_prefix}.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = \"Hello my friend\"\n",
    "tokenized = src_sp.EncodeAsIds(input_sentence)\n",
    "src = torch.LongTensor(pad_or_truncate(tokenized + [eos_id])).unsqueeze(0).to(device) # (1, L)\n",
    "e_mask = (src != pad_id).unsqueeze(1).to(device) # (1, 1, L)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_encoder(model, src, save_path):\n",
    "    print(src.shape)\n",
    "    e_output = model.encoder(src, e_mask) # (1, L, d_model)\n",
    "    torch.onnx.export(model.encoder, \n",
    "                      (src, e_mask), \n",
    "                      save_path, \n",
    "                      export_params=True, \n",
    "                      opset_version=16, \n",
    "                      do_constant_folding=True, \n",
    "                      input_names=['src', 'e_mask'], \n",
    "                      output_names=['e_output']) \n",
    "    return e_output\n",
    "    \n",
    "e_output = convert_encoder(model, src, os.path.join(ONNX_DIR, \"encoder.onnx\"))\n",
    "e_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.7998e+01, -1.9108e+01, -7.0844e+00,  ..., -1.9106e+01,\n",
       "          -1.9105e+01, -1.9106e+01],\n",
       "         [-1.1921e-07, -3.3358e+01, -2.0526e+01,  ..., -3.3356e+01,\n",
       "          -3.3356e+01, -3.3357e+01],\n",
       "         [-1.1921e-07, -3.3367e+01, -2.0561e+01,  ..., -3.3365e+01,\n",
       "          -3.3365e+01, -3.3366e+01],\n",
       "         ...,\n",
       "         [-2.3842e-07, -3.3306e+01, -2.0390e+01,  ..., -3.3304e+01,\n",
       "          -3.3304e+01, -3.3305e+01],\n",
       "         [-2.3842e-07, -3.3319e+01, -2.0412e+01,  ..., -3.3318e+01,\n",
       "          -3.3317e+01, -3.3318e+01],\n",
       "         [-2.3842e-07, -3.3333e+01, -2.0431e+01,  ..., -3.3331e+01,\n",
       "          -3.3331e+01, -3.3332e+01]]], grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_words = torch.LongTensor([pad_id] * seq_len).to(device) # (L)\n",
    "last_words[0] = sos_id # (L)\n",
    "cur_len = 1\n",
    "\n",
    "d_mask = (last_words.unsqueeze(0) != pad_id).unsqueeze(1).to(device) # (1, 1, L)\n",
    "nopeak_mask = torch.ones([1, seq_len, seq_len], dtype=torch.bool).to(device)  # (1, L, L)\n",
    "nopeak_mask = torch.tril(nopeak_mask)  # (1, L, L) to triangular shape\n",
    "d_mask = d_mask & nopeak_mask  # (1, L, L) padding false\n",
    "\n",
    "last_words_u = last_words.unsqueeze(0)\n",
    "\n",
    "def convert_decoder(model, last_words_u, e_output, e_mask, d_mask, save_path): \n",
    "    \n",
    "    decoder_output = model.decoder(\n",
    "                last_words_u,\n",
    "                e_output,\n",
    "                e_mask,\n",
    "                d_mask\n",
    "            ) # (1, L, d_model) \n",
    "    torch.onnx.export(model.decoder, \n",
    "                      (last_words_u, e_output, e_mask, d_mask), \n",
    "                      save_path, \n",
    "                      export_params=True, \n",
    "                      opset_version=16, \n",
    "                      do_constant_folding=True, \n",
    "                      input_names=['last_words_u', 'e_output', 'e_mask', 'd_mask'], \n",
    "                      output_names=['decoder_output']) \n",
    "    return decoder_output\n",
    "\n",
    "decoder_output = convert_decoder(model, last_words_u, e_output, e_mask, d_mask, os.path.join(ONNX_DIR, \"encoder.onnx\"))\n",
    "decoder_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and check model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import os\n",
    "\n",
    "\n",
    "encoder = onnx.load(os.path.join(ONNX_DIR, \"encoder.onnx\"))\n",
    "decoder = onnx.load(os.path.join(ONNX_DIR, \"decoder.onnx\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "onnx.checker.check_model(encoder)\n",
    "onnx.checker.check_model(decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx.helper.printable_graph(encoder.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "\n",
    "\n",
    "encoder = onnxruntime.InferenceSession(os.path.join(ONNX_DIR, \"encoder.onnx\"))\n",
    "decoder = onnxruntime.InferenceSession(os.path.join(ONNX_DIR, \"decoder.onnx\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class Translator():\n",
    "    def __init__(self, session) -> None:\n",
    "        self.encoder, self.decoder = session\n",
    "        self.src_sp = spm.SentencePieceProcessor()\n",
    "        self.trg_sp = spm.SentencePieceProcessor()\n",
    "        self.src_sp.load(f\"{SP_DIR}/{src_model_prefix}.model\")\n",
    "        self.trg_sp.load(f\"{SP_DIR}/{trg_model_prefix}.model\")\n",
    "        \n",
    "\n",
    "    def translate(self, input_sentence, method=\"greedy\"):\n",
    "        tokenized = self.src_sp.EncodeAsIds(input_sentence)\n",
    "        src = np.expand_dims(pad_or_truncate(tokenized + [eos_id]), axis=0).astype('int64') # (1, L)\n",
    "        e_mask = np.expand_dims((src != pad_id), axis=1) # (1, 1, L)\n",
    "       \n",
    "        e_output_input = { self.encoder.get_inputs()[0].name: src, self.encoder.get_inputs()[1].name: e_mask}\n",
    "        e_output = self.encoder.run(None, e_output_input)[0]\n",
    "        \n",
    "        if method == \"greedy\":\n",
    "            result = self.greedy_search(e_output, e_mask)\n",
    "        elif method == 'beam':\n",
    "        # print(\"Beam search selected.\")\n",
    "            result = self.beam_search(e_output, e_mask)\n",
    "        else:\n",
    "            raise ValueError(\"Method unsupported. Only support 'greedy' and 'beam' search\")\n",
    "\n",
    "        return result\n",
    "        \n",
    "    def greedy_search(self, e_output, e_mask):\n",
    "        last_words = [pad_id] * seq_len\n",
    "        last_words[0] = sos_id\n",
    "        cur_len = 1\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            lw_expand = np.expand_dims(last_words, axis=0)\n",
    "            d_mask = np.expand_dims((lw_expand != pad_id), axis=1) # (1, 1, L)\n",
    "            nopeak_mask = np.ones((1, seq_len, seq_len)).astype('bool')\n",
    "            nopeak_mask = np.tril(nopeak_mask) # (1, L, L) to triangular shape\n",
    "            d_mask = d_mask & nopeak_mask # (1, L, L) padding false\n",
    "\n",
    "            decoder_input = {self.decoder.get_inputs()[0].name: lw_expand,\n",
    "                                    self.decoder.get_inputs()[1].name: e_output,\n",
    "                                    self.decoder.get_inputs()[2].name: e_mask,\n",
    "                                    self.decoder.get_inputs()[3].name: d_mask}\n",
    "            decoder_output = self.decoder.run(None, decoder_input)[0] # (1, L, trg_vocab_size)\n",
    "\n",
    "            output = np.argmax(decoder_output, axis=-1)\n",
    "            last_word_id = output[0][i].item()\n",
    "\n",
    "            if i < seq_len-1:\n",
    "                last_words[i+1] = last_word_id\n",
    "                cur_len += 1\n",
    "            \n",
    "            if last_word_id == eos_id:\n",
    "                break\n",
    "\n",
    "        if last_words[-1] == pad_id:\n",
    "            decoded_output = last_words[1:cur_len]\n",
    "        else:\n",
    "            decoded_output = last_words[1:]\n",
    "        decoded_output = self.trg_sp.decode_ids(decoded_output)\n",
    "        \n",
    "        return decoded_output\n",
    "\n",
    "    def beam_search(self, e_output, e_mask):\n",
    "        cur_queue = PriorityQueue()\n",
    "        for k in range(beam_size):\n",
    "            cur_queue.put(BeamNode(sos_id, -0.0, [sos_id]))\n",
    "        \n",
    "        finished_count = 0\n",
    "        \n",
    "        for pos in range(seq_len):\n",
    "            new_queue = PriorityQueue()\n",
    "            for k in range(beam_size):\n",
    "                node = cur_queue.get()\n",
    "                if node.is_finished:\n",
    "                    new_queue.put(node)\n",
    "                else:\n",
    "                    trg_input = node.decoded + [pad_id] * (seq_len - len(node.decoded)) # (L)\n",
    "                    trg_input_expand = np.expand_dims(trg_input, axis=0) # (1, L)\n",
    "                    d_mask = np.expand_dims((trg_input_expand != pad_id), axis=1) # (1, 1, L)\n",
    "                    nopeak_mask = np.ones((1, seq_len, seq_len)).astype('bool')\n",
    "                    nopeak_mask = np.tril(nopeak_mask) # (1, L, L) to triangular shape\n",
    "                    d_mask = d_mask & nopeak_mask # (1, L, L) padding false\n",
    "                    \n",
    "                    decoder_input = {self.decoder.get_inputs()[0].name: trg_input_expand,\n",
    "                                    self.decoder.get_inputs()[1].name: e_output,\n",
    "                                    self.decoder.get_inputs()[2].name: e_mask,\n",
    "                                    self.decoder.get_inputs()[3].name: d_mask}\n",
    "                    output = self.decoder.run(None, decoder_input)[0] # (1, L, trg_vocab_size)\n",
    "\n",
    "                    # output = self.model.decoder(\n",
    "                    #     trg_input_expand,\n",
    "                    #     e_output,\n",
    "                    #     e_mask,\n",
    "                    #     d_mask\n",
    "                    # ) # (1, L, trg_vocab_size)\n",
    "                    \n",
    "                    output_prob, output_ind = self.topk(output[0][pos], k=beam_size, axis=-1)\n",
    "                    last_word_ids = output_ind.tolist() # (k)\n",
    "                    last_word_prob = output_prob.tolist() # (k)\n",
    "                    \n",
    "                    for i, idx in enumerate(last_word_ids):\n",
    "                        new_node = BeamNode(idx, -(-node.prob + last_word_prob[i]), node.decoded + [idx])\n",
    "                        if idx == eos_id:\n",
    "                            new_node.prob = new_node.prob / float(len(new_node.decoded))\n",
    "                            new_node.is_finished = True\n",
    "                            finished_count += 1\n",
    "                        new_queue.put(new_node)\n",
    "            \n",
    "            cur_queue = copy.deepcopy(new_queue)\n",
    "            \n",
    "            if finished_count == beam_size:\n",
    "                break\n",
    "        \n",
    "        decoded_output = cur_queue.get().decoded\n",
    "        \n",
    "        if decoded_output[-1] == eos_id:\n",
    "            decoded_output = decoded_output[1:-1]\n",
    "        else:\n",
    "            decoded_output = decoded_output[1:]\n",
    "            \n",
    "        return self.trg_sp.decode_ids(decoded_output)\n",
    "\n",
    "    def pad_or_truncate(self, tokenized_text):\n",
    "        if len(tokenized_text) < seq_len:\n",
    "            left = seq_len - len(tokenized_text)\n",
    "            padding = [pad_id] * left\n",
    "            tokenized_text += padding\n",
    "        else:\n",
    "            tokenized_text = tokenized_text[:seq_len]\n",
    "\n",
    "        return tokenized_text\n",
    "\n",
    "    def topk(self, array, k, axis=-1, sorted=True):\n",
    "        # Use np.argpartition is faster than np.argsort, but do not return the values in order\n",
    "        # We use array.take because you can specify the axis\n",
    "        partitioned_ind = (\n",
    "            np.argpartition(array, -k, axis=axis)\n",
    "            .take(indices=range(-k, 0), axis=axis)\n",
    "        )\n",
    "        # We use the newly selected indices to find the score of the top-k values\n",
    "        partitioned_scores = np.take_along_axis(array, partitioned_ind, axis=axis)\n",
    "        \n",
    "        if sorted:\n",
    "            # Since our top-k indices are not correctly ordered, we can sort them with argsort\n",
    "            # only if sorted=True (otherwise we keep it in an arbitrary order)\n",
    "            sorted_trunc_ind = np.flip(\n",
    "                np.argsort(partitioned_scores, axis=axis), axis=axis\n",
    "            )\n",
    "            \n",
    "            # We again use np.take_along_axis as we have an array of indices that we use to\n",
    "            # decide which values to select\n",
    "            ind = np.take_along_axis(partitioned_ind, sorted_trunc_ind, axis=axis)\n",
    "            scores = np.take_along_axis(partitioned_scores, sorted_trunc_ind, axis=axis)\n",
    "        else:\n",
    "            ind = partitioned_ind\n",
    "            scores = partitioned_scores\n",
    "        \n",
    "        return scores, ind\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Xin chào tên tôi là John'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session = (encoder, decoder)\n",
    "translator = Translator(session)\n",
    "translator.translate(\"Hello my name is John\", method=\"beam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
