{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dinh.trong.huy/nmt-data-envija/transformer-translator-pytorch\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unicodedata import name\n",
    "from tqdm import tqdm\n",
    "from constants import *\n",
    "from custom_data import *\n",
    "from transformer import *\n",
    "from data_structure import *\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(src_vocab_size=sp_src_vocab_size, trg_vocab_size=sp_trg_vocab_size, d_model=d_model).to(device)\n",
    "checkpoint = torch.load(f\"/home/dinh.trong.huy/nmt-data-envija/transformer-translator-pytorch/sq128_8_6_1_512_1024/javi2_mix_aug223k/ckpt_18_javi2.tar\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_src(text_list):\n",
    "    tokenized_list = []\n",
    "    for text in tqdm(text_list):\n",
    "        tokenized = src_sp.EncodeAsIds(text.strip())\n",
    "        tokenized_list.append(len(tokenized)+1)\n",
    "\n",
    "    return tokenized_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_truncate(tokenized_text):\n",
    "    if len(tokenized_text) < seq_len:\n",
    "        left = seq_len - len(tokenized_text)\n",
    "        padding = [pad_id] * left\n",
    "        tokenized_text += padding\n",
    "    else:\n",
    "        tokenized_text = tokenized_text[:seq_len]\n",
    "\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_sp = spm.SentencePieceProcessor()\n",
    "trg_sp = spm.SentencePieceProcessor()\n",
    "src_sp.load(f\"{SP_DIR}/{src_model_prefix}.model\")\n",
    "trg_sp.load(f\"{SP_DIR}/{trg_model_prefix}.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = \"Hello my friend\"\n",
    "tokenized = src_sp.EncodeAsIds(input_sentence)\n",
    "src = torch.LongTensor(pad_or_truncate(tokenized + [eos_id])).unsqueeze(0).to(device) # (1, L)\n",
    "e_mask = (src != pad_id).unsqueeze(1).to(device) # (1, 1, L)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128])\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3900,  0.6810, -0.3820,  ...,  1.1953, -0.2641, -0.1094],\n",
       "         [ 0.4117, -0.2298, -0.4929,  ...,  0.4729, -1.2949,  0.6192],\n",
       "         [ 0.5429, -0.2892, -0.1731,  ...,  0.3445, -1.1674,  1.1886],\n",
       "         ...,\n",
       "         [-0.2749, -0.0511, -0.4211,  ...,  0.1454, -0.3272,  0.5160],\n",
       "         [-0.1789, -0.1648, -0.6204,  ...,  0.2616, -0.2825,  0.4078],\n",
       "         [-0.2815, -0.2188, -0.2826,  ...,  0.2891, -0.3639,  0.5546]]],\n",
       "       device='cuda:1', grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_encoder(model, src, save_path):\n",
    "    print(src.shape)\n",
    "    e_output = model.encoder(src, e_mask) # (1, L, d_model)\n",
    "    torch.onnx.export(model.encoder, \n",
    "                      (src, e_mask), \n",
    "                      save_path, \n",
    "                      export_params=True, \n",
    "                      opset_version=16, \n",
    "                      do_constant_folding=True, \n",
    "                      input_names=['src', 'e_mask'], \n",
    "                      output_names=['e_output']) \n",
    "    return e_output\n",
    "    \n",
    "e_output = convert_encoder(model, src, os.path.join(ONNX_DIR, \"javi/encoder.onnx\"))\n",
    "e_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-15.0229, -18.0776,  -8.5911,  ..., -18.0765, -18.0762, -18.0772],\n",
       "         [  0.0000, -37.5369, -26.9227,  ..., -37.5363, -37.5363, -37.5370],\n",
       "         [  0.0000, -37.6758, -27.1081,  ..., -37.6755, -37.6747, -37.6762],\n",
       "         ...,\n",
       "         [  0.0000, -37.4011, -26.6607,  ..., -37.4001, -37.4001, -37.4003],\n",
       "         [  0.0000, -37.3517, -26.5201,  ..., -37.3515, -37.3512, -37.3516],\n",
       "         [  0.0000, -37.3702, -26.6146,  ..., -37.3701, -37.3693, -37.3701]]],\n",
       "       device='cuda:1', grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_words = torch.LongTensor([pad_id] * seq_len).to(device) # (L)\n",
    "last_words[0] = sos_id # (L)\n",
    "cur_len = 1\n",
    "\n",
    "d_mask = (last_words.unsqueeze(0) != pad_id).unsqueeze(1).to(device) # (1, 1, L)\n",
    "nopeak_mask = torch.ones([1, seq_len, seq_len], dtype=torch.bool).to(device)  # (1, L, L)\n",
    "nopeak_mask = torch.tril(nopeak_mask)  # (1, L, L) to triangular shape\n",
    "d_mask = d_mask & nopeak_mask  # (1, L, L) padding false\n",
    "\n",
    "last_words_u = last_words.unsqueeze(0)\n",
    "\n",
    "def convert_decoder(model, last_words_u, e_output, e_mask, d_mask, save_path): \n",
    "    \n",
    "    decoder_output = model.decoder(\n",
    "                last_words_u,\n",
    "                e_output,\n",
    "                e_mask,\n",
    "                d_mask\n",
    "            ) # (1, L, d_model) \n",
    "    torch.onnx.export(model.decoder, \n",
    "                      (last_words_u, e_output, e_mask, d_mask), \n",
    "                      save_path, \n",
    "                      export_params=True, \n",
    "                      opset_version=16, \n",
    "                      do_constant_folding=True, \n",
    "                      input_names=['last_words_u', 'e_output', 'e_mask', 'd_mask'], \n",
    "                      output_names=['decoder_output']) \n",
    "    return decoder_output\n",
    "\n",
    "decoder_output = convert_decoder(model, last_words_u, e_output, e_mask, d_mask, os.path.join(ONNX_DIR, \"javi/decoder.onnx\"))\n",
    "decoder_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and check model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import os\n",
    "\n",
    "\n",
    "encoder = onnx.load(os.path.join(ONNX_DIR, \"javi/encoder.onnx\"))\n",
    "decoder = onnx.load(os.path.join(ONNX_DIR, \"javi/decoder.onnx\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "onnx.checker.check_model(encoder)\n",
    "onnx.checker.check_model(decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graph torch_jit (\\n  %src[INT64, 1x128]\\n  %e_mask[BOOL, 1x1x128]\\n) initializers (\\n  %src_embedding.weight[FLOAT, 64000x512]\\n  %layers.0.layer_norm_1.layer.weight[FLOAT, 512]\\n  %layers.0.layer_norm_1.layer.bias[FLOAT, 512]\\n  %layers.0.multihead_attention.w_q.bias[FLOAT, 512]\\n  %layers.0.multihead_attention.w_k.bias[FLOAT, 512]\\n  %layers.0.multihead_attention.w_v.bias[FLOAT, 512]\\n  %layers.0.multihead_attention.w_0.bias[FLOAT, 512]\\n  %layers.0.layer_norm_2.layer.weight[FLOAT, 512]\\n  %layers.0.layer_norm_2.layer.bias[FLOAT, 512]\\n  %layers.0.feed_forward.linear_1.bias[FLOAT, 1024]\\n  %layers.0.feed_forward.linear_2.bias[FLOAT, 512]\\n  %layers.1.layer_norm_1.layer.weight[FLOAT, 512]\\n  %layers.1.layer_norm_1.layer.bias[FLOAT, 512]\\n  %layers.1.multihead_attention.w_q.bias[FLOAT, 512]\\n  %layers.1.multihead_attention.w_k.bias[FLOAT, 512]\\n  %layers.1.multihead_attention.w_v.bias[FLOAT, 512]\\n  %layers.1.multihead_attention.w_0.bias[FLOAT, 512]\\n  %layers.1.layer_norm_2.layer.weight[FLOAT, 512]\\n  %layers.1.layer_norm_2.layer.bias[FLOAT, 512]\\n  %layers.1.feed_forward.linear_1.bias[FLOAT, 1024]\\n  %layers.1.feed_forward.linear_2.bias[FLOAT, 512]\\n  %layers.2.layer_norm_1.layer.weight[FLOAT, 512]\\n  %layers.2.layer_norm_1.layer.bias[FLOAT, 512]\\n  %layers.2.multihead_attention.w_q.bias[FLOAT, 512]\\n  %layers.2.multihead_attention.w_k.bias[FLOAT, 512]\\n  %layers.2.multihead_attention.w_v.bias[FLOAT, 512]\\n  %layers.2.multihead_attention.w_0.bias[FLOAT, 512]\\n  %layers.2.layer_norm_2.layer.weight[FLOAT, 512]\\n  %layers.2.layer_norm_2.layer.bias[FLOAT, 512]\\n  %layers.2.feed_forward.linear_1.bias[FLOAT, 1024]\\n  %layers.2.feed_forward.linear_2.bias[FLOAT, 512]\\n  %layers.3.layer_norm_1.layer.weight[FLOAT, 512]\\n  %layers.3.layer_norm_1.layer.bias[FLOAT, 512]\\n  %layers.3.multihead_attention.w_q.bias[FLOAT, 512]\\n  %layers.3.multihead_attention.w_k.bias[FLOAT, 512]\\n  %layers.3.multihead_attention.w_v.bias[FLOAT, 512]\\n  %layers.3.multihead_attention.w_0.bias[FLOAT, 512]\\n  %layers.3.layer_norm_2.layer.weight[FLOAT, 512]\\n  %layers.3.layer_norm_2.layer.bias[FLOAT, 512]\\n  %layers.3.feed_forward.linear_1.bias[FLOAT, 1024]\\n  %layers.3.feed_forward.linear_2.bias[FLOAT, 512]\\n  %layers.4.layer_norm_1.layer.weight[FLOAT, 512]\\n  %layers.4.layer_norm_1.layer.bias[FLOAT, 512]\\n  %layers.4.multihead_attention.w_q.bias[FLOAT, 512]\\n  %layers.4.multihead_attention.w_k.bias[FLOAT, 512]\\n  %layers.4.multihead_attention.w_v.bias[FLOAT, 512]\\n  %layers.4.multihead_attention.w_0.bias[FLOAT, 512]\\n  %layers.4.layer_norm_2.layer.weight[FLOAT, 512]\\n  %layers.4.layer_norm_2.layer.bias[FLOAT, 512]\\n  %layers.4.feed_forward.linear_1.bias[FLOAT, 1024]\\n  %layers.4.feed_forward.linear_2.bias[FLOAT, 512]\\n  %layers.5.layer_norm_1.layer.weight[FLOAT, 512]\\n  %layers.5.layer_norm_1.layer.bias[FLOAT, 512]\\n  %layers.5.multihead_attention.w_q.bias[FLOAT, 512]\\n  %layers.5.multihead_attention.w_k.bias[FLOAT, 512]\\n  %layers.5.multihead_attention.w_v.bias[FLOAT, 512]\\n  %layers.5.multihead_attention.w_0.bias[FLOAT, 512]\\n  %layers.5.layer_norm_2.layer.weight[FLOAT, 512]\\n  %layers.5.layer_norm_2.layer.bias[FLOAT, 512]\\n  %layers.5.feed_forward.linear_1.bias[FLOAT, 1024]\\n  %layers.5.feed_forward.linear_2.bias[FLOAT, 512]\\n  %layer_norm.layer.weight[FLOAT, 512]\\n  %layer_norm.layer.bias[FLOAT, 512]\\n  %onnx::MatMul_690[FLOAT, 512x512]\\n  %onnx::MatMul_706[FLOAT, 512x512]\\n  %onnx::MatMul_707[FLOAT, 512x512]\\n  %onnx::MatMul_712[FLOAT, 512x512]\\n  %onnx::MatMul_713[FLOAT, 512x1024]\\n  %onnx::MatMul_714[FLOAT, 1024x512]\\n  %onnx::MatMul_715[FLOAT, 512x512]\\n  %onnx::MatMul_731[FLOAT, 512x512]\\n  %onnx::MatMul_732[FLOAT, 512x512]\\n  %onnx::MatMul_737[FLOAT, 512x512]\\n  %onnx::MatMul_738[FLOAT, 512x1024]\\n  %onnx::MatMul_739[FLOAT, 1024x512]\\n  %onnx::MatMul_740[FLOAT, 512x512]\\n  %onnx::MatMul_756[FLOAT, 512x512]\\n  %onnx::MatMul_757[FLOAT, 512x512]\\n  %onnx::MatMul_762[FLOAT, 512x512]\\n  %onnx::MatMul_763[FLOAT, 512x1024]\\n  %onnx::MatMul_764[FLOAT, 1024x512]\\n  %onnx::MatMul_765[FLOAT, 512x512]\\n  %onnx::MatMul_781[FLOAT, 512x512]\\n  %onnx::MatMul_782[FLOAT, 512x512]\\n  %onnx::MatMul_787[FLOAT, 512x512]\\n  %onnx::MatMul_788[FLOAT, 512x1024]\\n  %onnx::MatMul_789[FLOAT, 1024x512]\\n  %onnx::MatMul_790[FLOAT, 512x512]\\n  %onnx::MatMul_806[FLOAT, 512x512]\\n  %onnx::MatMul_807[FLOAT, 512x512]\\n  %onnx::MatMul_812[FLOAT, 512x512]\\n  %onnx::MatMul_813[FLOAT, 512x1024]\\n  %onnx::MatMul_814[FLOAT, 1024x512]\\n  %onnx::MatMul_815[FLOAT, 512x512]\\n  %onnx::MatMul_831[FLOAT, 512x512]\\n  %onnx::MatMul_832[FLOAT, 512x512]\\n  %onnx::MatMul_837[FLOAT, 512x512]\\n  %onnx::MatMul_838[FLOAT, 512x1024]\\n  %onnx::MatMul_839[FLOAT, 1024x512]\\n) {\\n  %/src_embedding/Gather_output_0 = Gather(%src_embedding.weight, %src)\\n  %/positional_encoder/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/positional_encoder/Mul_output_0 = Mul(%/src_embedding/Gather_output_0, %/positional_encoder/Constant_output_0)\\n  %/positional_encoder/Constant_1_output_0 = Constant[value = <Tensor>]()\\n  %/positional_encoder/Add_output_0 = Add(%/positional_encoder/Mul_output_0, %/positional_encoder/Constant_1_output_0)\\n  %/layers.0/layer_norm_1/layer/ReduceMean_output_0 = ReduceMean[axes = [-1]](%/positional_encoder/Add_output_0)\\n  %/layers.0/layer_norm_1/layer/Sub_output_0 = Sub(%/positional_encoder/Add_output_0, %/layers.0/layer_norm_1/layer/ReduceMean_output_0)\\n  %/layers.0/layer_norm_1/layer/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.0/layer_norm_1/layer/Pow_output_0 = Pow(%/layers.0/layer_norm_1/layer/Sub_output_0, %/layers.0/layer_norm_1/layer/Constant_output_0)\\n  %/layers.0/layer_norm_1/layer/ReduceMean_1_output_0 = ReduceMean[axes = [-1]](%/layers.0/layer_norm_1/layer/Pow_output_0)\\n  %/layers.0/layer_norm_1/layer/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.0/layer_norm_1/layer/Add_output_0 = Add(%/layers.0/layer_norm_1/layer/ReduceMean_1_output_0, %/layers.0/layer_norm_1/layer/Constant_1_output_0)\\n  %/layers.0/layer_norm_1/layer/Sqrt_output_0 = Sqrt(%/layers.0/layer_norm_1/layer/Add_output_0)\\n  %/layers.0/layer_norm_1/layer/Div_output_0 = Div(%/layers.0/layer_norm_1/layer/Sub_output_0, %/layers.0/layer_norm_1/layer/Sqrt_output_0)\\n  %/layers.0/layer_norm_1/layer/Mul_output_0 = Mul(%/layers.0/layer_norm_1/layer/Div_output_0, %layers.0.layer_norm_1.layer.weight)\\n  %/layers.0/layer_norm_1/layer/Add_1_output_0 = Add(%/layers.0/layer_norm_1/layer/Mul_output_0, %layers.0.layer_norm_1.layer.bias)\\n  %/layers.0/multihead_attention/w_q/MatMul_output_0 = MatMul(%/layers.0/layer_norm_1/layer/Add_1_output_0, %onnx::MatMul_690)\\n  %/layers.0/multihead_attention/w_q/Add_output_0 = Add(%layers.0.multihead_attention.w_q.bias, %/layers.0/multihead_attention/w_q/MatMul_output_0)\\n  %/layers.0/multihead_attention/Constant_output_0 = Constant[value = <Tensor>]()\\n  %/layers.0/multihead_attention/Constant_1_output_0 = Constant[value = <Tensor>]()\\n  %/layers.0/multihead_attention/Constant_2_output_0 = Constant[value = <Tensor>]()\\n  %/layers.0/multihead_attention/Reshape_output_0 = Reshape[allowzero = 0](%/layers.0/multihead_attention/w_q/Add_output_0, %/layers.0/multihead_attention/Constant_output_0)\\n  %/layers.0/multihead_attention/w_k/MatMul_output_0 = MatMul(%/layers.0/layer_norm_1/layer/Add_1_output_0, %onnx::MatMul_706)\\n  %/layers.0/multihead_attention/w_k/Add_output_0 = Add(%layers.0.multihead_attention.w_k.bias, %/layers.0/multihead_attention/w_k/MatMul_output_0)\\n  %/layers.0/multihead_attention/Reshape_1_output_0 = Reshape[allowzero = 0](%/layers.0/multihead_attention/w_k/Add_output_0, %/layers.0/multihead_attention/Constant_1_output_0)\\n  %/layers.0/multihead_attention/w_v/MatMul_output_0 = MatMul(%/layers.0/layer_norm_1/layer/Add_1_output_0, %onnx::MatMul_707)\\n  %/layers.0/multihead_attention/w_v/Add_output_0 = Add(%layers.0.multihead_attention.w_v.bias, %/layers.0/multihead_attention/w_v/MatMul_output_0)\\n  %/layers.0/multihead_attention/Reshape_2_output_0 = Reshape[allowzero = 0](%/layers.0/multihead_attention/w_v/Add_output_0, %/layers.0/multihead_attention/Constant_2_output_0)\\n  %/layers.0/multihead_attention/Transpose_output_0 = Transpose[perm = [0, 2, 1, 3]](%/layers.0/multihead_attention/Reshape_output_0)\\n  %/layers.0/multihead_attention/Transpose_1_output_0 = Transpose[perm = [0, 2, 1, 3]](%/layers.0/multihead_attention/Reshape_2_output_0)\\n  %/layers.0/multihead_attention/Transpose_2_output_0 = Transpose[perm = [0, 2, 3, 1]](%/layers.0/multihead_attention/Reshape_1_output_0)\\n  %/layers.0/multihead_attention/MatMul_output_0 = MatMul(%/layers.0/multihead_attention/Transpose_output_0, %/layers.0/multihead_attention/Transpose_2_output_0)\\n  %/layers.0/multihead_attention/Constant_3_output_0 = Constant[value = <Tensor>]()\\n  %/layers.0/multihead_attention/Unsqueeze_output_0 = Unsqueeze(%e_mask, %/layers.0/multihead_attention/Constant_3_output_0)\\n  %/layers.0/multihead_attention/Cast_output_0 = Cast[to = 7](%/layers.0/multihead_attention/Unsqueeze_output_0)\\n  %/layers.0/multihead_attention/Constant_4_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.0/multihead_attention/Equal_output_0 = Equal(%/layers.0/multihead_attention/Cast_output_0, %/layers.0/multihead_attention/Constant_4_output_0)\\n  %/layers.0/multihead_attention/Constant_5_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.0/multihead_attention/Div_output_0 = Div(%/layers.0/multihead_attention/MatMul_output_0, %/layers.0/multihead_attention/Constant_5_output_0)\\n  %/layers.0/multihead_attention/Cast_1_output_0 = Cast[to = 9](%/layers.0/multihead_attention/Equal_output_0)\\n  %/layers.0/multihead_attention/Constant_6_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.0/multihead_attention/Where_output_0 = Where(%/layers.0/multihead_attention/Cast_1_output_0, %/layers.0/multihead_attention/Constant_6_output_0, %/layers.0/multihead_attention/Div_output_0)\\n  %/layers.0/multihead_attention/attn_softmax/Softmax_output_0 = Softmax[axis = -1](%/layers.0/multihead_attention/Where_output_0)\\n  %/layers.0/multihead_attention/MatMul_1_output_0 = MatMul(%/layers.0/multihead_attention/attn_softmax/Softmax_output_0, %/layers.0/multihead_attention/Transpose_1_output_0)\\n  %/layers.0/multihead_attention/Transpose_3_output_0 = Transpose[perm = [0, 2, 1, 3]](%/layers.0/multihead_attention/MatMul_1_output_0)\\n  %/layers.0/multihead_attention/Constant_7_output_0 = Constant[value = <Tensor>]()\\n  %/layers.0/multihead_attention/Reshape_3_output_0 = Reshape[allowzero = 0](%/layers.0/multihead_attention/Transpose_3_output_0, %/layers.0/multihead_attention/Constant_7_output_0)\\n  %/layers.0/multihead_attention/w_0/MatMul_output_0 = MatMul(%/layers.0/multihead_attention/Reshape_3_output_0, %onnx::MatMul_712)\\n  %/layers.0/multihead_attention/w_0/Add_output_0 = Add(%layers.0.multihead_attention.w_0.bias, %/layers.0/multihead_attention/w_0/MatMul_output_0)\\n  %/layers.0/Add_output_0 = Add(%/positional_encoder/Add_output_0, %/layers.0/multihead_attention/w_0/Add_output_0)\\n  %/layers.0/layer_norm_2/layer/ReduceMean_output_0 = ReduceMean[axes = [-1]](%/layers.0/Add_output_0)\\n  %/layers.0/layer_norm_2/layer/Sub_output_0 = Sub(%/layers.0/Add_output_0, %/layers.0/layer_norm_2/layer/ReduceMean_output_0)\\n  %/layers.0/layer_norm_2/layer/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.0/layer_norm_2/layer/Pow_output_0 = Pow(%/layers.0/layer_norm_2/layer/Sub_output_0, %/layers.0/layer_norm_2/layer/Constant_output_0)\\n  %/layers.0/layer_norm_2/layer/ReduceMean_1_output_0 = ReduceMean[axes = [-1]](%/layers.0/layer_norm_2/layer/Pow_output_0)\\n  %/layers.0/layer_norm_2/layer/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.0/layer_norm_2/layer/Add_output_0 = Add(%/layers.0/layer_norm_2/layer/ReduceMean_1_output_0, %/layers.0/layer_norm_2/layer/Constant_1_output_0)\\n  %/layers.0/layer_norm_2/layer/Sqrt_output_0 = Sqrt(%/layers.0/layer_norm_2/layer/Add_output_0)\\n  %/layers.0/layer_norm_2/layer/Div_output_0 = Div(%/layers.0/layer_norm_2/layer/Sub_output_0, %/layers.0/layer_norm_2/layer/Sqrt_output_0)\\n  %/layers.0/layer_norm_2/layer/Mul_output_0 = Mul(%/layers.0/layer_norm_2/layer/Div_output_0, %layers.0.layer_norm_2.layer.weight)\\n  %/layers.0/layer_norm_2/layer/Add_1_output_0 = Add(%/layers.0/layer_norm_2/layer/Mul_output_0, %layers.0.layer_norm_2.layer.bias)\\n  %/layers.0/feed_forward/linear_1/MatMul_output_0 = MatMul(%/layers.0/layer_norm_2/layer/Add_1_output_0, %onnx::MatMul_713)\\n  %/layers.0/feed_forward/linear_1/Add_output_0 = Add(%layers.0.feed_forward.linear_1.bias, %/layers.0/feed_forward/linear_1/MatMul_output_0)\\n  %/layers.0/feed_forward/relu/Relu_output_0 = Relu(%/layers.0/feed_forward/linear_1/Add_output_0)\\n  %/layers.0/feed_forward/linear_2/MatMul_output_0 = MatMul(%/layers.0/feed_forward/relu/Relu_output_0, %onnx::MatMul_714)\\n  %/layers.0/feed_forward/linear_2/Add_output_0 = Add(%layers.0.feed_forward.linear_2.bias, %/layers.0/feed_forward/linear_2/MatMul_output_0)\\n  %/layers.0/Add_1_output_0 = Add(%/layers.0/Add_output_0, %/layers.0/feed_forward/linear_2/Add_output_0)\\n  %/layers.1/layer_norm_1/layer/ReduceMean_output_0 = ReduceMean[axes = [-1]](%/layers.0/Add_1_output_0)\\n  %/layers.1/layer_norm_1/layer/Sub_output_0 = Sub(%/layers.0/Add_1_output_0, %/layers.1/layer_norm_1/layer/ReduceMean_output_0)\\n  %/layers.1/layer_norm_1/layer/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.1/layer_norm_1/layer/Pow_output_0 = Pow(%/layers.1/layer_norm_1/layer/Sub_output_0, %/layers.1/layer_norm_1/layer/Constant_output_0)\\n  %/layers.1/layer_norm_1/layer/ReduceMean_1_output_0 = ReduceMean[axes = [-1]](%/layers.1/layer_norm_1/layer/Pow_output_0)\\n  %/layers.1/layer_norm_1/layer/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.1/layer_norm_1/layer/Add_output_0 = Add(%/layers.1/layer_norm_1/layer/ReduceMean_1_output_0, %/layers.1/layer_norm_1/layer/Constant_1_output_0)\\n  %/layers.1/layer_norm_1/layer/Sqrt_output_0 = Sqrt(%/layers.1/layer_norm_1/layer/Add_output_0)\\n  %/layers.1/layer_norm_1/layer/Div_output_0 = Div(%/layers.1/layer_norm_1/layer/Sub_output_0, %/layers.1/layer_norm_1/layer/Sqrt_output_0)\\n  %/layers.1/layer_norm_1/layer/Mul_output_0 = Mul(%/layers.1/layer_norm_1/layer/Div_output_0, %layers.1.layer_norm_1.layer.weight)\\n  %/layers.1/layer_norm_1/layer/Add_1_output_0 = Add(%/layers.1/layer_norm_1/layer/Mul_output_0, %layers.1.layer_norm_1.layer.bias)\\n  %/layers.1/multihead_attention/w_q/MatMul_output_0 = MatMul(%/layers.1/layer_norm_1/layer/Add_1_output_0, %onnx::MatMul_715)\\n  %/layers.1/multihead_attention/w_q/Add_output_0 = Add(%layers.1.multihead_attention.w_q.bias, %/layers.1/multihead_attention/w_q/MatMul_output_0)\\n  %/layers.1/multihead_attention/Constant_output_0 = Constant[value = <Tensor>]()\\n  %/layers.1/multihead_attention/Constant_1_output_0 = Constant[value = <Tensor>]()\\n  %/layers.1/multihead_attention/Constant_2_output_0 = Constant[value = <Tensor>]()\\n  %/layers.1/multihead_attention/Reshape_output_0 = Reshape[allowzero = 0](%/layers.1/multihead_attention/w_q/Add_output_0, %/layers.1/multihead_attention/Constant_output_0)\\n  %/layers.1/multihead_attention/w_k/MatMul_output_0 = MatMul(%/layers.1/layer_norm_1/layer/Add_1_output_0, %onnx::MatMul_731)\\n  %/layers.1/multihead_attention/w_k/Add_output_0 = Add(%layers.1.multihead_attention.w_k.bias, %/layers.1/multihead_attention/w_k/MatMul_output_0)\\n  %/layers.1/multihead_attention/Reshape_1_output_0 = Reshape[allowzero = 0](%/layers.1/multihead_attention/w_k/Add_output_0, %/layers.1/multihead_attention/Constant_1_output_0)\\n  %/layers.1/multihead_attention/w_v/MatMul_output_0 = MatMul(%/layers.1/layer_norm_1/layer/Add_1_output_0, %onnx::MatMul_732)\\n  %/layers.1/multihead_attention/w_v/Add_output_0 = Add(%layers.1.multihead_attention.w_v.bias, %/layers.1/multihead_attention/w_v/MatMul_output_0)\\n  %/layers.1/multihead_attention/Reshape_2_output_0 = Reshape[allowzero = 0](%/layers.1/multihead_attention/w_v/Add_output_0, %/layers.1/multihead_attention/Constant_2_output_0)\\n  %/layers.1/multihead_attention/Transpose_output_0 = Transpose[perm = [0, 2, 1, 3]](%/layers.1/multihead_attention/Reshape_output_0)\\n  %/layers.1/multihead_attention/Transpose_1_output_0 = Transpose[perm = [0, 2, 1, 3]](%/layers.1/multihead_attention/Reshape_2_output_0)\\n  %/layers.1/multihead_attention/Transpose_2_output_0 = Transpose[perm = [0, 2, 3, 1]](%/layers.1/multihead_attention/Reshape_1_output_0)\\n  %/layers.1/multihead_attention/MatMul_output_0 = MatMul(%/layers.1/multihead_attention/Transpose_output_0, %/layers.1/multihead_attention/Transpose_2_output_0)\\n  %/layers.1/multihead_attention/Constant_3_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.1/multihead_attention/Div_output_0 = Div(%/layers.1/multihead_attention/MatMul_output_0, %/layers.1/multihead_attention/Constant_3_output_0)\\n  %/layers.1/multihead_attention/Cast_output_0 = Cast[to = 9](%/layers.0/multihead_attention/Equal_output_0)\\n  %/layers.1/multihead_attention/Constant_4_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.1/multihead_attention/Where_output_0 = Where(%/layers.1/multihead_attention/Cast_output_0, %/layers.1/multihead_attention/Constant_4_output_0, %/layers.1/multihead_attention/Div_output_0)\\n  %/layers.1/multihead_attention/attn_softmax/Softmax_output_0 = Softmax[axis = -1](%/layers.1/multihead_attention/Where_output_0)\\n  %/layers.1/multihead_attention/MatMul_1_output_0 = MatMul(%/layers.1/multihead_attention/attn_softmax/Softmax_output_0, %/layers.1/multihead_attention/Transpose_1_output_0)\\n  %/layers.1/multihead_attention/Transpose_3_output_0 = Transpose[perm = [0, 2, 1, 3]](%/layers.1/multihead_attention/MatMul_1_output_0)\\n  %/layers.1/multihead_attention/Constant_5_output_0 = Constant[value = <Tensor>]()\\n  %/layers.1/multihead_attention/Reshape_3_output_0 = Reshape[allowzero = 0](%/layers.1/multihead_attention/Transpose_3_output_0, %/layers.1/multihead_attention/Constant_5_output_0)\\n  %/layers.1/multihead_attention/w_0/MatMul_output_0 = MatMul(%/layers.1/multihead_attention/Reshape_3_output_0, %onnx::MatMul_737)\\n  %/layers.1/multihead_attention/w_0/Add_output_0 = Add(%layers.1.multihead_attention.w_0.bias, %/layers.1/multihead_attention/w_0/MatMul_output_0)\\n  %/layers.1/Add_output_0 = Add(%/layers.0/Add_1_output_0, %/layers.1/multihead_attention/w_0/Add_output_0)\\n  %/layers.1/layer_norm_2/layer/ReduceMean_output_0 = ReduceMean[axes = [-1]](%/layers.1/Add_output_0)\\n  %/layers.1/layer_norm_2/layer/Sub_output_0 = Sub(%/layers.1/Add_output_0, %/layers.1/layer_norm_2/layer/ReduceMean_output_0)\\n  %/layers.1/layer_norm_2/layer/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.1/layer_norm_2/layer/Pow_output_0 = Pow(%/layers.1/layer_norm_2/layer/Sub_output_0, %/layers.1/layer_norm_2/layer/Constant_output_0)\\n  %/layers.1/layer_norm_2/layer/ReduceMean_1_output_0 = ReduceMean[axes = [-1]](%/layers.1/layer_norm_2/layer/Pow_output_0)\\n  %/layers.1/layer_norm_2/layer/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.1/layer_norm_2/layer/Add_output_0 = Add(%/layers.1/layer_norm_2/layer/ReduceMean_1_output_0, %/layers.1/layer_norm_2/layer/Constant_1_output_0)\\n  %/layers.1/layer_norm_2/layer/Sqrt_output_0 = Sqrt(%/layers.1/layer_norm_2/layer/Add_output_0)\\n  %/layers.1/layer_norm_2/layer/Div_output_0 = Div(%/layers.1/layer_norm_2/layer/Sub_output_0, %/layers.1/layer_norm_2/layer/Sqrt_output_0)\\n  %/layers.1/layer_norm_2/layer/Mul_output_0 = Mul(%/layers.1/layer_norm_2/layer/Div_output_0, %layers.1.layer_norm_2.layer.weight)\\n  %/layers.1/layer_norm_2/layer/Add_1_output_0 = Add(%/layers.1/layer_norm_2/layer/Mul_output_0, %layers.1.layer_norm_2.layer.bias)\\n  %/layers.1/feed_forward/linear_1/MatMul_output_0 = MatMul(%/layers.1/layer_norm_2/layer/Add_1_output_0, %onnx::MatMul_738)\\n  %/layers.1/feed_forward/linear_1/Add_output_0 = Add(%layers.1.feed_forward.linear_1.bias, %/layers.1/feed_forward/linear_1/MatMul_output_0)\\n  %/layers.1/feed_forward/relu/Relu_output_0 = Relu(%/layers.1/feed_forward/linear_1/Add_output_0)\\n  %/layers.1/feed_forward/linear_2/MatMul_output_0 = MatMul(%/layers.1/feed_forward/relu/Relu_output_0, %onnx::MatMul_739)\\n  %/layers.1/feed_forward/linear_2/Add_output_0 = Add(%layers.1.feed_forward.linear_2.bias, %/layers.1/feed_forward/linear_2/MatMul_output_0)\\n  %/layers.1/Add_1_output_0 = Add(%/layers.1/Add_output_0, %/layers.1/feed_forward/linear_2/Add_output_0)\\n  %/layers.2/layer_norm_1/layer/ReduceMean_output_0 = ReduceMean[axes = [-1]](%/layers.1/Add_1_output_0)\\n  %/layers.2/layer_norm_1/layer/Sub_output_0 = Sub(%/layers.1/Add_1_output_0, %/layers.2/layer_norm_1/layer/ReduceMean_output_0)\\n  %/layers.2/layer_norm_1/layer/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.2/layer_norm_1/layer/Pow_output_0 = Pow(%/layers.2/layer_norm_1/layer/Sub_output_0, %/layers.2/layer_norm_1/layer/Constant_output_0)\\n  %/layers.2/layer_norm_1/layer/ReduceMean_1_output_0 = ReduceMean[axes = [-1]](%/layers.2/layer_norm_1/layer/Pow_output_0)\\n  %/layers.2/layer_norm_1/layer/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.2/layer_norm_1/layer/Add_output_0 = Add(%/layers.2/layer_norm_1/layer/ReduceMean_1_output_0, %/layers.2/layer_norm_1/layer/Constant_1_output_0)\\n  %/layers.2/layer_norm_1/layer/Sqrt_output_0 = Sqrt(%/layers.2/layer_norm_1/layer/Add_output_0)\\n  %/layers.2/layer_norm_1/layer/Div_output_0 = Div(%/layers.2/layer_norm_1/layer/Sub_output_0, %/layers.2/layer_norm_1/layer/Sqrt_output_0)\\n  %/layers.2/layer_norm_1/layer/Mul_output_0 = Mul(%/layers.2/layer_norm_1/layer/Div_output_0, %layers.2.layer_norm_1.layer.weight)\\n  %/layers.2/layer_norm_1/layer/Add_1_output_0 = Add(%/layers.2/layer_norm_1/layer/Mul_output_0, %layers.2.layer_norm_1.layer.bias)\\n  %/layers.2/multihead_attention/w_q/MatMul_output_0 = MatMul(%/layers.2/layer_norm_1/layer/Add_1_output_0, %onnx::MatMul_740)\\n  %/layers.2/multihead_attention/w_q/Add_output_0 = Add(%layers.2.multihead_attention.w_q.bias, %/layers.2/multihead_attention/w_q/MatMul_output_0)\\n  %/layers.2/multihead_attention/Constant_output_0 = Constant[value = <Tensor>]()\\n  %/layers.2/multihead_attention/Constant_1_output_0 = Constant[value = <Tensor>]()\\n  %/layers.2/multihead_attention/Constant_2_output_0 = Constant[value = <Tensor>]()\\n  %/layers.2/multihead_attention/Reshape_output_0 = Reshape[allowzero = 0](%/layers.2/multihead_attention/w_q/Add_output_0, %/layers.2/multihead_attention/Constant_output_0)\\n  %/layers.2/multihead_attention/w_k/MatMul_output_0 = MatMul(%/layers.2/layer_norm_1/layer/Add_1_output_0, %onnx::MatMul_756)\\n  %/layers.2/multihead_attention/w_k/Add_output_0 = Add(%layers.2.multihead_attention.w_k.bias, %/layers.2/multihead_attention/w_k/MatMul_output_0)\\n  %/layers.2/multihead_attention/Reshape_1_output_0 = Reshape[allowzero = 0](%/layers.2/multihead_attention/w_k/Add_output_0, %/layers.2/multihead_attention/Constant_1_output_0)\\n  %/layers.2/multihead_attention/w_v/MatMul_output_0 = MatMul(%/layers.2/layer_norm_1/layer/Add_1_output_0, %onnx::MatMul_757)\\n  %/layers.2/multihead_attention/w_v/Add_output_0 = Add(%layers.2.multihead_attention.w_v.bias, %/layers.2/multihead_attention/w_v/MatMul_output_0)\\n  %/layers.2/multihead_attention/Reshape_2_output_0 = Reshape[allowzero = 0](%/layers.2/multihead_attention/w_v/Add_output_0, %/layers.2/multihead_attention/Constant_2_output_0)\\n  %/layers.2/multihead_attention/Transpose_output_0 = Transpose[perm = [0, 2, 1, 3]](%/layers.2/multihead_attention/Reshape_output_0)\\n  %/layers.2/multihead_attention/Transpose_1_output_0 = Transpose[perm = [0, 2, 1, 3]](%/layers.2/multihead_attention/Reshape_2_output_0)\\n  %/layers.2/multihead_attention/Transpose_2_output_0 = Transpose[perm = [0, 2, 3, 1]](%/layers.2/multihead_attention/Reshape_1_output_0)\\n  %/layers.2/multihead_attention/MatMul_output_0 = MatMul(%/layers.2/multihead_attention/Transpose_output_0, %/layers.2/multihead_attention/Transpose_2_output_0)\\n  %/layers.2/multihead_attention/Constant_3_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.2/multihead_attention/Div_output_0 = Div(%/layers.2/multihead_attention/MatMul_output_0, %/layers.2/multihead_attention/Constant_3_output_0)\\n  %/layers.2/multihead_attention/Cast_output_0 = Cast[to = 9](%/layers.0/multihead_attention/Equal_output_0)\\n  %/layers.2/multihead_attention/Constant_4_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.2/multihead_attention/Where_output_0 = Where(%/layers.2/multihead_attention/Cast_output_0, %/layers.2/multihead_attention/Constant_4_output_0, %/layers.2/multihead_attention/Div_output_0)\\n  %/layers.2/multihead_attention/attn_softmax/Softmax_output_0 = Softmax[axis = -1](%/layers.2/multihead_attention/Where_output_0)\\n  %/layers.2/multihead_attention/MatMul_1_output_0 = MatMul(%/layers.2/multihead_attention/attn_softmax/Softmax_output_0, %/layers.2/multihead_attention/Transpose_1_output_0)\\n  %/layers.2/multihead_attention/Transpose_3_output_0 = Transpose[perm = [0, 2, 1, 3]](%/layers.2/multihead_attention/MatMul_1_output_0)\\n  %/layers.2/multihead_attention/Constant_5_output_0 = Constant[value = <Tensor>]()\\n  %/layers.2/multihead_attention/Reshape_3_output_0 = Reshape[allowzero = 0](%/layers.2/multihead_attention/Transpose_3_output_0, %/layers.2/multihead_attention/Constant_5_output_0)\\n  %/layers.2/multihead_attention/w_0/MatMul_output_0 = MatMul(%/layers.2/multihead_attention/Reshape_3_output_0, %onnx::MatMul_762)\\n  %/layers.2/multihead_attention/w_0/Add_output_0 = Add(%layers.2.multihead_attention.w_0.bias, %/layers.2/multihead_attention/w_0/MatMul_output_0)\\n  %/layers.2/Add_output_0 = Add(%/layers.1/Add_1_output_0, %/layers.2/multihead_attention/w_0/Add_output_0)\\n  %/layers.2/layer_norm_2/layer/ReduceMean_output_0 = ReduceMean[axes = [-1]](%/layers.2/Add_output_0)\\n  %/layers.2/layer_norm_2/layer/Sub_output_0 = Sub(%/layers.2/Add_output_0, %/layers.2/layer_norm_2/layer/ReduceMean_output_0)\\n  %/layers.2/layer_norm_2/layer/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.2/layer_norm_2/layer/Pow_output_0 = Pow(%/layers.2/layer_norm_2/layer/Sub_output_0, %/layers.2/layer_norm_2/layer/Constant_output_0)\\n  %/layers.2/layer_norm_2/layer/ReduceMean_1_output_0 = ReduceMean[axes = [-1]](%/layers.2/layer_norm_2/layer/Pow_output_0)\\n  %/layers.2/layer_norm_2/layer/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.2/layer_norm_2/layer/Add_output_0 = Add(%/layers.2/layer_norm_2/layer/ReduceMean_1_output_0, %/layers.2/layer_norm_2/layer/Constant_1_output_0)\\n  %/layers.2/layer_norm_2/layer/Sqrt_output_0 = Sqrt(%/layers.2/layer_norm_2/layer/Add_output_0)\\n  %/layers.2/layer_norm_2/layer/Div_output_0 = Div(%/layers.2/layer_norm_2/layer/Sub_output_0, %/layers.2/layer_norm_2/layer/Sqrt_output_0)\\n  %/layers.2/layer_norm_2/layer/Mul_output_0 = Mul(%/layers.2/layer_norm_2/layer/Div_output_0, %layers.2.layer_norm_2.layer.weight)\\n  %/layers.2/layer_norm_2/layer/Add_1_output_0 = Add(%/layers.2/layer_norm_2/layer/Mul_output_0, %layers.2.layer_norm_2.layer.bias)\\n  %/layers.2/feed_forward/linear_1/MatMul_output_0 = MatMul(%/layers.2/layer_norm_2/layer/Add_1_output_0, %onnx::MatMul_763)\\n  %/layers.2/feed_forward/linear_1/Add_output_0 = Add(%layers.2.feed_forward.linear_1.bias, %/layers.2/feed_forward/linear_1/MatMul_output_0)\\n  %/layers.2/feed_forward/relu/Relu_output_0 = Relu(%/layers.2/feed_forward/linear_1/Add_output_0)\\n  %/layers.2/feed_forward/linear_2/MatMul_output_0 = MatMul(%/layers.2/feed_forward/relu/Relu_output_0, %onnx::MatMul_764)\\n  %/layers.2/feed_forward/linear_2/Add_output_0 = Add(%layers.2.feed_forward.linear_2.bias, %/layers.2/feed_forward/linear_2/MatMul_output_0)\\n  %/layers.2/Add_1_output_0 = Add(%/layers.2/Add_output_0, %/layers.2/feed_forward/linear_2/Add_output_0)\\n  %/layers.3/layer_norm_1/layer/ReduceMean_output_0 = ReduceMean[axes = [-1]](%/layers.2/Add_1_output_0)\\n  %/layers.3/layer_norm_1/layer/Sub_output_0 = Sub(%/layers.2/Add_1_output_0, %/layers.3/layer_norm_1/layer/ReduceMean_output_0)\\n  %/layers.3/layer_norm_1/layer/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.3/layer_norm_1/layer/Pow_output_0 = Pow(%/layers.3/layer_norm_1/layer/Sub_output_0, %/layers.3/layer_norm_1/layer/Constant_output_0)\\n  %/layers.3/layer_norm_1/layer/ReduceMean_1_output_0 = ReduceMean[axes = [-1]](%/layers.3/layer_norm_1/layer/Pow_output_0)\\n  %/layers.3/layer_norm_1/layer/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.3/layer_norm_1/layer/Add_output_0 = Add(%/layers.3/layer_norm_1/layer/ReduceMean_1_output_0, %/layers.3/layer_norm_1/layer/Constant_1_output_0)\\n  %/layers.3/layer_norm_1/layer/Sqrt_output_0 = Sqrt(%/layers.3/layer_norm_1/layer/Add_output_0)\\n  %/layers.3/layer_norm_1/layer/Div_output_0 = Div(%/layers.3/layer_norm_1/layer/Sub_output_0, %/layers.3/layer_norm_1/layer/Sqrt_output_0)\\n  %/layers.3/layer_norm_1/layer/Mul_output_0 = Mul(%/layers.3/layer_norm_1/layer/Div_output_0, %layers.3.layer_norm_1.layer.weight)\\n  %/layers.3/layer_norm_1/layer/Add_1_output_0 = Add(%/layers.3/layer_norm_1/layer/Mul_output_0, %layers.3.layer_norm_1.layer.bias)\\n  %/layers.3/multihead_attention/w_q/MatMul_output_0 = MatMul(%/layers.3/layer_norm_1/layer/Add_1_output_0, %onnx::MatMul_765)\\n  %/layers.3/multihead_attention/w_q/Add_output_0 = Add(%layers.3.multihead_attention.w_q.bias, %/layers.3/multihead_attention/w_q/MatMul_output_0)\\n  %/layers.3/multihead_attention/Constant_output_0 = Constant[value = <Tensor>]()\\n  %/layers.3/multihead_attention/Constant_1_output_0 = Constant[value = <Tensor>]()\\n  %/layers.3/multihead_attention/Constant_2_output_0 = Constant[value = <Tensor>]()\\n  %/layers.3/multihead_attention/Reshape_output_0 = Reshape[allowzero = 0](%/layers.3/multihead_attention/w_q/Add_output_0, %/layers.3/multihead_attention/Constant_output_0)\\n  %/layers.3/multihead_attention/w_k/MatMul_output_0 = MatMul(%/layers.3/layer_norm_1/layer/Add_1_output_0, %onnx::MatMul_781)\\n  %/layers.3/multihead_attention/w_k/Add_output_0 = Add(%layers.3.multihead_attention.w_k.bias, %/layers.3/multihead_attention/w_k/MatMul_output_0)\\n  %/layers.3/multihead_attention/Reshape_1_output_0 = Reshape[allowzero = 0](%/layers.3/multihead_attention/w_k/Add_output_0, %/layers.3/multihead_attention/Constant_1_output_0)\\n  %/layers.3/multihead_attention/w_v/MatMul_output_0 = MatMul(%/layers.3/layer_norm_1/layer/Add_1_output_0, %onnx::MatMul_782)\\n  %/layers.3/multihead_attention/w_v/Add_output_0 = Add(%layers.3.multihead_attention.w_v.bias, %/layers.3/multihead_attention/w_v/MatMul_output_0)\\n  %/layers.3/multihead_attention/Reshape_2_output_0 = Reshape[allowzero = 0](%/layers.3/multihead_attention/w_v/Add_output_0, %/layers.3/multihead_attention/Constant_2_output_0)\\n  %/layers.3/multihead_attention/Transpose_output_0 = Transpose[perm = [0, 2, 1, 3]](%/layers.3/multihead_attention/Reshape_output_0)\\n  %/layers.3/multihead_attention/Transpose_1_output_0 = Transpose[perm = [0, 2, 1, 3]](%/layers.3/multihead_attention/Reshape_2_output_0)\\n  %/layers.3/multihead_attention/Transpose_2_output_0 = Transpose[perm = [0, 2, 3, 1]](%/layers.3/multihead_attention/Reshape_1_output_0)\\n  %/layers.3/multihead_attention/MatMul_output_0 = MatMul(%/layers.3/multihead_attention/Transpose_output_0, %/layers.3/multihead_attention/Transpose_2_output_0)\\n  %/layers.3/multihead_attention/Constant_3_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.3/multihead_attention/Div_output_0 = Div(%/layers.3/multihead_attention/MatMul_output_0, %/layers.3/multihead_attention/Constant_3_output_0)\\n  %/layers.3/multihead_attention/Cast_output_0 = Cast[to = 9](%/layers.0/multihead_attention/Equal_output_0)\\n  %/layers.3/multihead_attention/Constant_4_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.3/multihead_attention/Where_output_0 = Where(%/layers.3/multihead_attention/Cast_output_0, %/layers.3/multihead_attention/Constant_4_output_0, %/layers.3/multihead_attention/Div_output_0)\\n  %/layers.3/multihead_attention/attn_softmax/Softmax_output_0 = Softmax[axis = -1](%/layers.3/multihead_attention/Where_output_0)\\n  %/layers.3/multihead_attention/MatMul_1_output_0 = MatMul(%/layers.3/multihead_attention/attn_softmax/Softmax_output_0, %/layers.3/multihead_attention/Transpose_1_output_0)\\n  %/layers.3/multihead_attention/Transpose_3_output_0 = Transpose[perm = [0, 2, 1, 3]](%/layers.3/multihead_attention/MatMul_1_output_0)\\n  %/layers.3/multihead_attention/Constant_5_output_0 = Constant[value = <Tensor>]()\\n  %/layers.3/multihead_attention/Reshape_3_output_0 = Reshape[allowzero = 0](%/layers.3/multihead_attention/Transpose_3_output_0, %/layers.3/multihead_attention/Constant_5_output_0)\\n  %/layers.3/multihead_attention/w_0/MatMul_output_0 = MatMul(%/layers.3/multihead_attention/Reshape_3_output_0, %onnx::MatMul_787)\\n  %/layers.3/multihead_attention/w_0/Add_output_0 = Add(%layers.3.multihead_attention.w_0.bias, %/layers.3/multihead_attention/w_0/MatMul_output_0)\\n  %/layers.3/Add_output_0 = Add(%/layers.2/Add_1_output_0, %/layers.3/multihead_attention/w_0/Add_output_0)\\n  %/layers.3/layer_norm_2/layer/ReduceMean_output_0 = ReduceMean[axes = [-1]](%/layers.3/Add_output_0)\\n  %/layers.3/layer_norm_2/layer/Sub_output_0 = Sub(%/layers.3/Add_output_0, %/layers.3/layer_norm_2/layer/ReduceMean_output_0)\\n  %/layers.3/layer_norm_2/layer/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.3/layer_norm_2/layer/Pow_output_0 = Pow(%/layers.3/layer_norm_2/layer/Sub_output_0, %/layers.3/layer_norm_2/layer/Constant_output_0)\\n  %/layers.3/layer_norm_2/layer/ReduceMean_1_output_0 = ReduceMean[axes = [-1]](%/layers.3/layer_norm_2/layer/Pow_output_0)\\n  %/layers.3/layer_norm_2/layer/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.3/layer_norm_2/layer/Add_output_0 = Add(%/layers.3/layer_norm_2/layer/ReduceMean_1_output_0, %/layers.3/layer_norm_2/layer/Constant_1_output_0)\\n  %/layers.3/layer_norm_2/layer/Sqrt_output_0 = Sqrt(%/layers.3/layer_norm_2/layer/Add_output_0)\\n  %/layers.3/layer_norm_2/layer/Div_output_0 = Div(%/layers.3/layer_norm_2/layer/Sub_output_0, %/layers.3/layer_norm_2/layer/Sqrt_output_0)\\n  %/layers.3/layer_norm_2/layer/Mul_output_0 = Mul(%/layers.3/layer_norm_2/layer/Div_output_0, %layers.3.layer_norm_2.layer.weight)\\n  %/layers.3/layer_norm_2/layer/Add_1_output_0 = Add(%/layers.3/layer_norm_2/layer/Mul_output_0, %layers.3.layer_norm_2.layer.bias)\\n  %/layers.3/feed_forward/linear_1/MatMul_output_0 = MatMul(%/layers.3/layer_norm_2/layer/Add_1_output_0, %onnx::MatMul_788)\\n  %/layers.3/feed_forward/linear_1/Add_output_0 = Add(%layers.3.feed_forward.linear_1.bias, %/layers.3/feed_forward/linear_1/MatMul_output_0)\\n  %/layers.3/feed_forward/relu/Relu_output_0 = Relu(%/layers.3/feed_forward/linear_1/Add_output_0)\\n  %/layers.3/feed_forward/linear_2/MatMul_output_0 = MatMul(%/layers.3/feed_forward/relu/Relu_output_0, %onnx::MatMul_789)\\n  %/layers.3/feed_forward/linear_2/Add_output_0 = Add(%layers.3.feed_forward.linear_2.bias, %/layers.3/feed_forward/linear_2/MatMul_output_0)\\n  %/layers.3/Add_1_output_0 = Add(%/layers.3/Add_output_0, %/layers.3/feed_forward/linear_2/Add_output_0)\\n  %/layers.4/layer_norm_1/layer/ReduceMean_output_0 = ReduceMean[axes = [-1]](%/layers.3/Add_1_output_0)\\n  %/layers.4/layer_norm_1/layer/Sub_output_0 = Sub(%/layers.3/Add_1_output_0, %/layers.4/layer_norm_1/layer/ReduceMean_output_0)\\n  %/layers.4/layer_norm_1/layer/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.4/layer_norm_1/layer/Pow_output_0 = Pow(%/layers.4/layer_norm_1/layer/Sub_output_0, %/layers.4/layer_norm_1/layer/Constant_output_0)\\n  %/layers.4/layer_norm_1/layer/ReduceMean_1_output_0 = ReduceMean[axes = [-1]](%/layers.4/layer_norm_1/layer/Pow_output_0)\\n  %/layers.4/layer_norm_1/layer/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.4/layer_norm_1/layer/Add_output_0 = Add(%/layers.4/layer_norm_1/layer/ReduceMean_1_output_0, %/layers.4/layer_norm_1/layer/Constant_1_output_0)\\n  %/layers.4/layer_norm_1/layer/Sqrt_output_0 = Sqrt(%/layers.4/layer_norm_1/layer/Add_output_0)\\n  %/layers.4/layer_norm_1/layer/Div_output_0 = Div(%/layers.4/layer_norm_1/layer/Sub_output_0, %/layers.4/layer_norm_1/layer/Sqrt_output_0)\\n  %/layers.4/layer_norm_1/layer/Mul_output_0 = Mul(%/layers.4/layer_norm_1/layer/Div_output_0, %layers.4.layer_norm_1.layer.weight)\\n  %/layers.4/layer_norm_1/layer/Add_1_output_0 = Add(%/layers.4/layer_norm_1/layer/Mul_output_0, %layers.4.layer_norm_1.layer.bias)\\n  %/layers.4/multihead_attention/w_q/MatMul_output_0 = MatMul(%/layers.4/layer_norm_1/layer/Add_1_output_0, %onnx::MatMul_790)\\n  %/layers.4/multihead_attention/w_q/Add_output_0 = Add(%layers.4.multihead_attention.w_q.bias, %/layers.4/multihead_attention/w_q/MatMul_output_0)\\n  %/layers.4/multihead_attention/Constant_output_0 = Constant[value = <Tensor>]()\\n  %/layers.4/multihead_attention/Constant_1_output_0 = Constant[value = <Tensor>]()\\n  %/layers.4/multihead_attention/Constant_2_output_0 = Constant[value = <Tensor>]()\\n  %/layers.4/multihead_attention/Reshape_output_0 = Reshape[allowzero = 0](%/layers.4/multihead_attention/w_q/Add_output_0, %/layers.4/multihead_attention/Constant_output_0)\\n  %/layers.4/multihead_attention/w_k/MatMul_output_0 = MatMul(%/layers.4/layer_norm_1/layer/Add_1_output_0, %onnx::MatMul_806)\\n  %/layers.4/multihead_attention/w_k/Add_output_0 = Add(%layers.4.multihead_attention.w_k.bias, %/layers.4/multihead_attention/w_k/MatMul_output_0)\\n  %/layers.4/multihead_attention/Reshape_1_output_0 = Reshape[allowzero = 0](%/layers.4/multihead_attention/w_k/Add_output_0, %/layers.4/multihead_attention/Constant_1_output_0)\\n  %/layers.4/multihead_attention/w_v/MatMul_output_0 = MatMul(%/layers.4/layer_norm_1/layer/Add_1_output_0, %onnx::MatMul_807)\\n  %/layers.4/multihead_attention/w_v/Add_output_0 = Add(%layers.4.multihead_attention.w_v.bias, %/layers.4/multihead_attention/w_v/MatMul_output_0)\\n  %/layers.4/multihead_attention/Reshape_2_output_0 = Reshape[allowzero = 0](%/layers.4/multihead_attention/w_v/Add_output_0, %/layers.4/multihead_attention/Constant_2_output_0)\\n  %/layers.4/multihead_attention/Transpose_output_0 = Transpose[perm = [0, 2, 1, 3]](%/layers.4/multihead_attention/Reshape_output_0)\\n  %/layers.4/multihead_attention/Transpose_1_output_0 = Transpose[perm = [0, 2, 1, 3]](%/layers.4/multihead_attention/Reshape_2_output_0)\\n  %/layers.4/multihead_attention/Transpose_2_output_0 = Transpose[perm = [0, 2, 3, 1]](%/layers.4/multihead_attention/Reshape_1_output_0)\\n  %/layers.4/multihead_attention/MatMul_output_0 = MatMul(%/layers.4/multihead_attention/Transpose_output_0, %/layers.4/multihead_attention/Transpose_2_output_0)\\n  %/layers.4/multihead_attention/Constant_3_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.4/multihead_attention/Div_output_0 = Div(%/layers.4/multihead_attention/MatMul_output_0, %/layers.4/multihead_attention/Constant_3_output_0)\\n  %/layers.4/multihead_attention/Cast_output_0 = Cast[to = 9](%/layers.0/multihead_attention/Equal_output_0)\\n  %/layers.4/multihead_attention/Constant_4_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.4/multihead_attention/Where_output_0 = Where(%/layers.4/multihead_attention/Cast_output_0, %/layers.4/multihead_attention/Constant_4_output_0, %/layers.4/multihead_attention/Div_output_0)\\n  %/layers.4/multihead_attention/attn_softmax/Softmax_output_0 = Softmax[axis = -1](%/layers.4/multihead_attention/Where_output_0)\\n  %/layers.4/multihead_attention/MatMul_1_output_0 = MatMul(%/layers.4/multihead_attention/attn_softmax/Softmax_output_0, %/layers.4/multihead_attention/Transpose_1_output_0)\\n  %/layers.4/multihead_attention/Transpose_3_output_0 = Transpose[perm = [0, 2, 1, 3]](%/layers.4/multihead_attention/MatMul_1_output_0)\\n  %/layers.4/multihead_attention/Constant_5_output_0 = Constant[value = <Tensor>]()\\n  %/layers.4/multihead_attention/Reshape_3_output_0 = Reshape[allowzero = 0](%/layers.4/multihead_attention/Transpose_3_output_0, %/layers.4/multihead_attention/Constant_5_output_0)\\n  %/layers.4/multihead_attention/w_0/MatMul_output_0 = MatMul(%/layers.4/multihead_attention/Reshape_3_output_0, %onnx::MatMul_812)\\n  %/layers.4/multihead_attention/w_0/Add_output_0 = Add(%layers.4.multihead_attention.w_0.bias, %/layers.4/multihead_attention/w_0/MatMul_output_0)\\n  %/layers.4/Add_output_0 = Add(%/layers.3/Add_1_output_0, %/layers.4/multihead_attention/w_0/Add_output_0)\\n  %/layers.4/layer_norm_2/layer/ReduceMean_output_0 = ReduceMean[axes = [-1]](%/layers.4/Add_output_0)\\n  %/layers.4/layer_norm_2/layer/Sub_output_0 = Sub(%/layers.4/Add_output_0, %/layers.4/layer_norm_2/layer/ReduceMean_output_0)\\n  %/layers.4/layer_norm_2/layer/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.4/layer_norm_2/layer/Pow_output_0 = Pow(%/layers.4/layer_norm_2/layer/Sub_output_0, %/layers.4/layer_norm_2/layer/Constant_output_0)\\n  %/layers.4/layer_norm_2/layer/ReduceMean_1_output_0 = ReduceMean[axes = [-1]](%/layers.4/layer_norm_2/layer/Pow_output_0)\\n  %/layers.4/layer_norm_2/layer/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.4/layer_norm_2/layer/Add_output_0 = Add(%/layers.4/layer_norm_2/layer/ReduceMean_1_output_0, %/layers.4/layer_norm_2/layer/Constant_1_output_0)\\n  %/layers.4/layer_norm_2/layer/Sqrt_output_0 = Sqrt(%/layers.4/layer_norm_2/layer/Add_output_0)\\n  %/layers.4/layer_norm_2/layer/Div_output_0 = Div(%/layers.4/layer_norm_2/layer/Sub_output_0, %/layers.4/layer_norm_2/layer/Sqrt_output_0)\\n  %/layers.4/layer_norm_2/layer/Mul_output_0 = Mul(%/layers.4/layer_norm_2/layer/Div_output_0, %layers.4.layer_norm_2.layer.weight)\\n  %/layers.4/layer_norm_2/layer/Add_1_output_0 = Add(%/layers.4/layer_norm_2/layer/Mul_output_0, %layers.4.layer_norm_2.layer.bias)\\n  %/layers.4/feed_forward/linear_1/MatMul_output_0 = MatMul(%/layers.4/layer_norm_2/layer/Add_1_output_0, %onnx::MatMul_813)\\n  %/layers.4/feed_forward/linear_1/Add_output_0 = Add(%layers.4.feed_forward.linear_1.bias, %/layers.4/feed_forward/linear_1/MatMul_output_0)\\n  %/layers.4/feed_forward/relu/Relu_output_0 = Relu(%/layers.4/feed_forward/linear_1/Add_output_0)\\n  %/layers.4/feed_forward/linear_2/MatMul_output_0 = MatMul(%/layers.4/feed_forward/relu/Relu_output_0, %onnx::MatMul_814)\\n  %/layers.4/feed_forward/linear_2/Add_output_0 = Add(%layers.4.feed_forward.linear_2.bias, %/layers.4/feed_forward/linear_2/MatMul_output_0)\\n  %/layers.4/Add_1_output_0 = Add(%/layers.4/Add_output_0, %/layers.4/feed_forward/linear_2/Add_output_0)\\n  %/layers.5/layer_norm_1/layer/ReduceMean_output_0 = ReduceMean[axes = [-1]](%/layers.4/Add_1_output_0)\\n  %/layers.5/layer_norm_1/layer/Sub_output_0 = Sub(%/layers.4/Add_1_output_0, %/layers.5/layer_norm_1/layer/ReduceMean_output_0)\\n  %/layers.5/layer_norm_1/layer/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.5/layer_norm_1/layer/Pow_output_0 = Pow(%/layers.5/layer_norm_1/layer/Sub_output_0, %/layers.5/layer_norm_1/layer/Constant_output_0)\\n  %/layers.5/layer_norm_1/layer/ReduceMean_1_output_0 = ReduceMean[axes = [-1]](%/layers.5/layer_norm_1/layer/Pow_output_0)\\n  %/layers.5/layer_norm_1/layer/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.5/layer_norm_1/layer/Add_output_0 = Add(%/layers.5/layer_norm_1/layer/ReduceMean_1_output_0, %/layers.5/layer_norm_1/layer/Constant_1_output_0)\\n  %/layers.5/layer_norm_1/layer/Sqrt_output_0 = Sqrt(%/layers.5/layer_norm_1/layer/Add_output_0)\\n  %/layers.5/layer_norm_1/layer/Div_output_0 = Div(%/layers.5/layer_norm_1/layer/Sub_output_0, %/layers.5/layer_norm_1/layer/Sqrt_output_0)\\n  %/layers.5/layer_norm_1/layer/Mul_output_0 = Mul(%/layers.5/layer_norm_1/layer/Div_output_0, %layers.5.layer_norm_1.layer.weight)\\n  %/layers.5/layer_norm_1/layer/Add_1_output_0 = Add(%/layers.5/layer_norm_1/layer/Mul_output_0, %layers.5.layer_norm_1.layer.bias)\\n  %/layers.5/multihead_attention/w_q/MatMul_output_0 = MatMul(%/layers.5/layer_norm_1/layer/Add_1_output_0, %onnx::MatMul_815)\\n  %/layers.5/multihead_attention/w_q/Add_output_0 = Add(%layers.5.multihead_attention.w_q.bias, %/layers.5/multihead_attention/w_q/MatMul_output_0)\\n  %/layers.5/multihead_attention/Constant_output_0 = Constant[value = <Tensor>]()\\n  %/layers.5/multihead_attention/Constant_1_output_0 = Constant[value = <Tensor>]()\\n  %/layers.5/multihead_attention/Constant_2_output_0 = Constant[value = <Tensor>]()\\n  %/layers.5/multihead_attention/Reshape_output_0 = Reshape[allowzero = 0](%/layers.5/multihead_attention/w_q/Add_output_0, %/layers.5/multihead_attention/Constant_output_0)\\n  %/layers.5/multihead_attention/w_k/MatMul_output_0 = MatMul(%/layers.5/layer_norm_1/layer/Add_1_output_0, %onnx::MatMul_831)\\n  %/layers.5/multihead_attention/w_k/Add_output_0 = Add(%layers.5.multihead_attention.w_k.bias, %/layers.5/multihead_attention/w_k/MatMul_output_0)\\n  %/layers.5/multihead_attention/Reshape_1_output_0 = Reshape[allowzero = 0](%/layers.5/multihead_attention/w_k/Add_output_0, %/layers.5/multihead_attention/Constant_1_output_0)\\n  %/layers.5/multihead_attention/w_v/MatMul_output_0 = MatMul(%/layers.5/layer_norm_1/layer/Add_1_output_0, %onnx::MatMul_832)\\n  %/layers.5/multihead_attention/w_v/Add_output_0 = Add(%layers.5.multihead_attention.w_v.bias, %/layers.5/multihead_attention/w_v/MatMul_output_0)\\n  %/layers.5/multihead_attention/Reshape_2_output_0 = Reshape[allowzero = 0](%/layers.5/multihead_attention/w_v/Add_output_0, %/layers.5/multihead_attention/Constant_2_output_0)\\n  %/layers.5/multihead_attention/Transpose_output_0 = Transpose[perm = [0, 2, 1, 3]](%/layers.5/multihead_attention/Reshape_output_0)\\n  %/layers.5/multihead_attention/Transpose_1_output_0 = Transpose[perm = [0, 2, 1, 3]](%/layers.5/multihead_attention/Reshape_2_output_0)\\n  %/layers.5/multihead_attention/Transpose_2_output_0 = Transpose[perm = [0, 2, 3, 1]](%/layers.5/multihead_attention/Reshape_1_output_0)\\n  %/layers.5/multihead_attention/MatMul_output_0 = MatMul(%/layers.5/multihead_attention/Transpose_output_0, %/layers.5/multihead_attention/Transpose_2_output_0)\\n  %/layers.5/multihead_attention/Constant_3_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.5/multihead_attention/Div_output_0 = Div(%/layers.5/multihead_attention/MatMul_output_0, %/layers.5/multihead_attention/Constant_3_output_0)\\n  %/layers.5/multihead_attention/Cast_output_0 = Cast[to = 9](%/layers.0/multihead_attention/Equal_output_0)\\n  %/layers.5/multihead_attention/Constant_4_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.5/multihead_attention/Where_output_0 = Where(%/layers.5/multihead_attention/Cast_output_0, %/layers.5/multihead_attention/Constant_4_output_0, %/layers.5/multihead_attention/Div_output_0)\\n  %/layers.5/multihead_attention/attn_softmax/Softmax_output_0 = Softmax[axis = -1](%/layers.5/multihead_attention/Where_output_0)\\n  %/layers.5/multihead_attention/MatMul_1_output_0 = MatMul(%/layers.5/multihead_attention/attn_softmax/Softmax_output_0, %/layers.5/multihead_attention/Transpose_1_output_0)\\n  %/layers.5/multihead_attention/Transpose_3_output_0 = Transpose[perm = [0, 2, 1, 3]](%/layers.5/multihead_attention/MatMul_1_output_0)\\n  %/layers.5/multihead_attention/Constant_5_output_0 = Constant[value = <Tensor>]()\\n  %/layers.5/multihead_attention/Reshape_3_output_0 = Reshape[allowzero = 0](%/layers.5/multihead_attention/Transpose_3_output_0, %/layers.5/multihead_attention/Constant_5_output_0)\\n  %/layers.5/multihead_attention/w_0/MatMul_output_0 = MatMul(%/layers.5/multihead_attention/Reshape_3_output_0, %onnx::MatMul_837)\\n  %/layers.5/multihead_attention/w_0/Add_output_0 = Add(%layers.5.multihead_attention.w_0.bias, %/layers.5/multihead_attention/w_0/MatMul_output_0)\\n  %/layers.5/Add_output_0 = Add(%/layers.4/Add_1_output_0, %/layers.5/multihead_attention/w_0/Add_output_0)\\n  %/layers.5/layer_norm_2/layer/ReduceMean_output_0 = ReduceMean[axes = [-1]](%/layers.5/Add_output_0)\\n  %/layers.5/layer_norm_2/layer/Sub_output_0 = Sub(%/layers.5/Add_output_0, %/layers.5/layer_norm_2/layer/ReduceMean_output_0)\\n  %/layers.5/layer_norm_2/layer/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.5/layer_norm_2/layer/Pow_output_0 = Pow(%/layers.5/layer_norm_2/layer/Sub_output_0, %/layers.5/layer_norm_2/layer/Constant_output_0)\\n  %/layers.5/layer_norm_2/layer/ReduceMean_1_output_0 = ReduceMean[axes = [-1]](%/layers.5/layer_norm_2/layer/Pow_output_0)\\n  %/layers.5/layer_norm_2/layer/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layers.5/layer_norm_2/layer/Add_output_0 = Add(%/layers.5/layer_norm_2/layer/ReduceMean_1_output_0, %/layers.5/layer_norm_2/layer/Constant_1_output_0)\\n  %/layers.5/layer_norm_2/layer/Sqrt_output_0 = Sqrt(%/layers.5/layer_norm_2/layer/Add_output_0)\\n  %/layers.5/layer_norm_2/layer/Div_output_0 = Div(%/layers.5/layer_norm_2/layer/Sub_output_0, %/layers.5/layer_norm_2/layer/Sqrt_output_0)\\n  %/layers.5/layer_norm_2/layer/Mul_output_0 = Mul(%/layers.5/layer_norm_2/layer/Div_output_0, %layers.5.layer_norm_2.layer.weight)\\n  %/layers.5/layer_norm_2/layer/Add_1_output_0 = Add(%/layers.5/layer_norm_2/layer/Mul_output_0, %layers.5.layer_norm_2.layer.bias)\\n  %/layers.5/feed_forward/linear_1/MatMul_output_0 = MatMul(%/layers.5/layer_norm_2/layer/Add_1_output_0, %onnx::MatMul_838)\\n  %/layers.5/feed_forward/linear_1/Add_output_0 = Add(%layers.5.feed_forward.linear_1.bias, %/layers.5/feed_forward/linear_1/MatMul_output_0)\\n  %/layers.5/feed_forward/relu/Relu_output_0 = Relu(%/layers.5/feed_forward/linear_1/Add_output_0)\\n  %/layers.5/feed_forward/linear_2/MatMul_output_0 = MatMul(%/layers.5/feed_forward/relu/Relu_output_0, %onnx::MatMul_839)\\n  %/layers.5/feed_forward/linear_2/Add_output_0 = Add(%layers.5.feed_forward.linear_2.bias, %/layers.5/feed_forward/linear_2/MatMul_output_0)\\n  %/layers.5/Add_1_output_0 = Add(%/layers.5/Add_output_0, %/layers.5/feed_forward/linear_2/Add_output_0)\\n  %/layer_norm/layer/ReduceMean_output_0 = ReduceMean[axes = [-1]](%/layers.5/Add_1_output_0)\\n  %/layer_norm/layer/Sub_output_0 = Sub(%/layers.5/Add_1_output_0, %/layer_norm/layer/ReduceMean_output_0)\\n  %/layer_norm/layer/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layer_norm/layer/Pow_output_0 = Pow(%/layer_norm/layer/Sub_output_0, %/layer_norm/layer/Constant_output_0)\\n  %/layer_norm/layer/ReduceMean_1_output_0 = ReduceMean[axes = [-1]](%/layer_norm/layer/Pow_output_0)\\n  %/layer_norm/layer/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\\n  %/layer_norm/layer/Add_output_0 = Add(%/layer_norm/layer/ReduceMean_1_output_0, %/layer_norm/layer/Constant_1_output_0)\\n  %/layer_norm/layer/Sqrt_output_0 = Sqrt(%/layer_norm/layer/Add_output_0)\\n  %/layer_norm/layer/Div_output_0 = Div(%/layer_norm/layer/Sub_output_0, %/layer_norm/layer/Sqrt_output_0)\\n  %/layer_norm/layer/Mul_output_0 = Mul(%/layer_norm/layer/Div_output_0, %layer_norm.layer.weight)\\n  %e_output = Add(%/layer_norm/layer/Mul_output_0, %layer_norm.layer.bias)\\n  return %e_output\\n}'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx.helper.printable_graph(encoder.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "\n",
    "\n",
    "encoder = onnxruntime.InferenceSession(os.path.join(ONNX_DIR, \"javi/encoder.onnx\"), providers=['CPUExecutionProvider'])\n",
    "decoder = onnxruntime.InferenceSession(os.path.join(ONNX_DIR, \"javi/decoder.onnx\"), providers=['CPUExecutionProvider'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class Translator():\n",
    "    def __init__(self, session) -> None:\n",
    "        self.encoder, self.decoder = session\n",
    "        self.src_sp = spm.SentencePieceProcessor()\n",
    "        self.trg_sp = spm.SentencePieceProcessor()\n",
    "        self.src_sp.load(f\"{SP_DIR}/{src_model_prefix}.model\")\n",
    "        self.trg_sp.load(f\"{SP_DIR}/{trg_model_prefix}.model\")\n",
    "        \n",
    "\n",
    "    def translate(self, input_sentence, method=\"greedy\"):\n",
    "        tokenized = self.src_sp.EncodeAsIds(input_sentence)\n",
    "        src = np.expand_dims(pad_or_truncate(tokenized + [eos_id]), axis=0).astype('int64') # (1, L)\n",
    "        e_mask = np.expand_dims((src != pad_id), axis=1) # (1, 1, L)\n",
    "       \n",
    "        e_output_input = { self.encoder.get_inputs()[0].name: src, self.encoder.get_inputs()[1].name: e_mask}\n",
    "        e_output = self.encoder.run(None, e_output_input)[0]\n",
    "        \n",
    "        if method == \"greedy\":\n",
    "            result = self.greedy_search(e_output, e_mask)\n",
    "        elif method == 'beam':\n",
    "        # print(\"Beam search selected.\")\n",
    "            result = self.beam_search(e_output, e_mask)\n",
    "        else:\n",
    "            raise ValueError(\"Method unsupported. Only support 'greedy' and 'beam' search\")\n",
    "\n",
    "        return result\n",
    "        \n",
    "    def greedy_search(self, e_output, e_mask):\n",
    "        last_words = [pad_id] * seq_len\n",
    "        last_words[0] = sos_id\n",
    "        cur_len = 1\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            lw_expand = np.expand_dims(last_words, axis=0)\n",
    "            d_mask = np.expand_dims((lw_expand != pad_id), axis=1) # (1, 1, L)\n",
    "            nopeak_mask = np.ones((1, seq_len, seq_len)).astype('bool')\n",
    "            nopeak_mask = np.tril(nopeak_mask) # (1, L, L) to triangular shape\n",
    "            d_mask = d_mask & nopeak_mask # (1, L, L) padding false\n",
    "\n",
    "            decoder_input = {self.decoder.get_inputs()[0].name: lw_expand,\n",
    "                                    self.decoder.get_inputs()[1].name: e_output,\n",
    "                                    self.decoder.get_inputs()[2].name: e_mask,\n",
    "                                    self.decoder.get_inputs()[3].name: d_mask}\n",
    "            decoder_output = self.decoder.run(None, decoder_input)[0] # (1, L, trg_vocab_size)\n",
    "\n",
    "            output = np.argmax(decoder_output, axis=-1)\n",
    "            last_word_id = output[0][i].item()\n",
    "\n",
    "            if i < seq_len-1:\n",
    "                last_words[i+1] = last_word_id\n",
    "                cur_len += 1\n",
    "            \n",
    "            if last_word_id == eos_id:\n",
    "                break\n",
    "\n",
    "        if last_words[-1] == pad_id:\n",
    "            decoded_output = last_words[1:cur_len]\n",
    "        else:\n",
    "            decoded_output = last_words[1:]\n",
    "        decoded_output = self.trg_sp.decode_ids(decoded_output)\n",
    "        \n",
    "        return decoded_output\n",
    "\n",
    "    def beam_search(self, e_output, e_mask):\n",
    "        cur_queue = PriorityQueue()\n",
    "        for k in range(beam_size):\n",
    "            cur_queue.put(BeamNode(sos_id, -0.0, [sos_id]))\n",
    "        \n",
    "        finished_count = 0\n",
    "        \n",
    "        for pos in range(seq_len):\n",
    "            new_queue = PriorityQueue()\n",
    "            for k in range(beam_size):\n",
    "                node = cur_queue.get()\n",
    "                if node.is_finished:\n",
    "                    new_queue.put(node)\n",
    "                else:\n",
    "                    trg_input = node.decoded + [pad_id] * (seq_len - len(node.decoded)) # (L)\n",
    "                    trg_input_expand = np.expand_dims(trg_input, axis=0) # (1, L)\n",
    "                    d_mask = np.expand_dims((trg_input_expand != pad_id), axis=1) # (1, 1, L)\n",
    "                    nopeak_mask = np.ones((1, seq_len, seq_len)).astype('bool')\n",
    "                    nopeak_mask = np.tril(nopeak_mask) # (1, L, L) to triangular shape\n",
    "                    d_mask = d_mask & nopeak_mask # (1, L, L) padding false\n",
    "                    \n",
    "                    decoder_input = {self.decoder.get_inputs()[0].name: trg_input_expand,\n",
    "                                    self.decoder.get_inputs()[1].name: e_output,\n",
    "                                    self.decoder.get_inputs()[2].name: e_mask,\n",
    "                                    self.decoder.get_inputs()[3].name: d_mask}\n",
    "                    output = self.decoder.run(None, decoder_input)[0] # (1, L, trg_vocab_size)\n",
    "\n",
    "                    # output = self.model.decoder(\n",
    "                    #     trg_input_expand,\n",
    "                    #     e_output,\n",
    "                    #     e_mask,\n",
    "                    #     d_mask\n",
    "                    # ) # (1, L, trg_vocab_size)\n",
    "                    \n",
    "                    output_prob, output_ind = self.topk(output[0][pos], k=beam_size, axis=-1)\n",
    "                    last_word_ids = output_ind.tolist() # (k)\n",
    "                    last_word_prob = output_prob.tolist() # (k)\n",
    "                    \n",
    "                    for i, idx in enumerate(last_word_ids):\n",
    "                        new_node = BeamNode(idx, -(-node.prob + last_word_prob[i]), node.decoded + [idx])\n",
    "                        if idx == eos_id:\n",
    "                            new_node.prob = new_node.prob / float(len(new_node.decoded))\n",
    "                            new_node.is_finished = True\n",
    "                            finished_count += 1\n",
    "                        new_queue.put(new_node)\n",
    "            \n",
    "            cur_queue = copy.deepcopy(new_queue)\n",
    "            \n",
    "            if finished_count == beam_size:\n",
    "                break\n",
    "        \n",
    "        decoded_output = cur_queue.get().decoded\n",
    "        \n",
    "        if decoded_output[-1] == eos_id:\n",
    "            decoded_output = decoded_output[1:-1]\n",
    "        else:\n",
    "            decoded_output = decoded_output[1:]\n",
    "            \n",
    "        return self.trg_sp.decode_ids(decoded_output)\n",
    "\n",
    "    def pad_or_truncate(self, tokenized_text):\n",
    "        if len(tokenized_text) < seq_len:\n",
    "            left = seq_len - len(tokenized_text)\n",
    "            padding = [pad_id] * left\n",
    "            tokenized_text += padding\n",
    "        else:\n",
    "            tokenized_text = tokenized_text[:seq_len]\n",
    "\n",
    "        return tokenized_text\n",
    "\n",
    "    def topk(self, array, k, axis=-1, sorted=True):\n",
    "        # Use np.argpartition is faster than np.argsort, but do not return the values in order\n",
    "        # We use array.take because you can specify the axis\n",
    "        partitioned_ind = (\n",
    "            np.argpartition(array, -k, axis=axis)\n",
    "            .take(indices=range(-k, 0), axis=axis)\n",
    "        )\n",
    "        # We use the newly selected indices to find the score of the top-k values\n",
    "        partitioned_scores = np.take_along_axis(array, partitioned_ind, axis=axis)\n",
    "        \n",
    "        if sorted:\n",
    "            # Since our top-k indices are not correctly ordered, we can sort them with argsort\n",
    "            # only if sorted=True (otherwise we keep it in an arbitrary order)\n",
    "            sorted_trunc_ind = np.flip(\n",
    "                np.argsort(partitioned_scores, axis=axis), axis=axis\n",
    "            )\n",
    "            \n",
    "            # We again use np.take_along_axis as we have an array of indices that we use to\n",
    "            # decide which values to select\n",
    "            ind = np.take_along_axis(partitioned_ind, sorted_trunc_ind, axis=axis)\n",
    "            scores = np.take_along_axis(partitioned_scores, sorted_trunc_ind, axis=axis)\n",
    "        else:\n",
    "            ind = partitioned_ind\n",
    "            scores = partitioned_scores\n",
    "        \n",
    "        return scores, ind\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tn ti l yamada'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session = (encoder, decoder)\n",
    "translator = Translator(session)\n",
    "translator.translate(\"Yamada\", method=\"beam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
